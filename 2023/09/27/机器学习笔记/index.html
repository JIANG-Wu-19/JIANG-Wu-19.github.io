<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="机器学习笔记, J&amp;Ocean BLOG">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>机器学习笔记 | J&amp;Ocean BLOG</title>
    <link rel="icon" type="image/x-icon, image/vnd.microsoft.icon" href="/favicon.ico">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 6.3.0"></head>



   <style>
    body{
       background-image: url(https://github.com/JIANG-Wu-19/JIANG-Wu-19/blob/master/99759389_p0.png?raw=true);
       background-repeat:no-repeat;
       background-size:cover;
       background-attachment:fixed;
    }
</style>



<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.ico" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">J&amp;Ocean BLOG</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.ico" class="logo-img circle responsive-img">
        
        <div class="logo-name">J&amp;Ocean BLOG</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">机器学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">机器学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E7%AC%94%E8%AE%B0/" class="post-category">
                                笔记
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-09-27
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    70 分
                </div>
                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h1><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><h3 id="ML框架"><a href="#ML框架" class="headerlink" title="ML框架"></a>ML框架</h3><p><img src="/imgs/ml/1.png" alt="image-20230913084005731"></p>
<h3 id="types-of-learning-problems"><a href="#types-of-learning-problems" class="headerlink" title="types of learning problems"></a>types of learning problems</h3><ul>
<li>监督学习</li>
<li>无监督学习<ul>
<li>自监督学习</li>
</ul>
</li>
<li>半监督学习</li>
<li>迁移学习</li>
<li>主动学习</li>
<li>强化学习</li>
<li>元学习</li>
</ul>
<h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><h3 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h3><ul>
<li><p><strong>输入特征</strong></p>
<p>$x^{(i)} \in R^{n+1},i&#x3D;1,2,···,m$</p>
</li>
<li><p><strong>输出</strong></p>
<p>$y^{(i)} \in R$</p>
</li>
<li><p><strong>参数</strong></p>
<p>$\theta &#x3D; R^{n+1}$</p>
</li>
<li><p>假设$h_{\theta}(x):R^{n+1} \to R$</p>
</li>
<li><p><strong>损失函数</strong></p>
<p>$\ell :R \times R \to R_+$</p>
<p>满足</p>
<ul>
<li>非负：不存在负损失</li>
<li>如果预测结果$h_{\theta}(x)$与给定的y差别小，则损失小，反之则损失大</li>
</ul>
<p>平方损失：</p>
<p>$\ell(h_{\theta}(x),y)&#x3D;(h_{\theta}(x)-y)^2$</p>
</li>
</ul>
<h4 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a><strong>三要素</strong></h4><ul>
<li><p>假设：$h_{\theta}(x)&#x3D; \theta_0+\theta_1x$，其中参数为$\theta_0,\theta_1$</p>
</li>
<li><p>目标函数：</p>
<p>$J\left(\theta_0, \theta_1\right)&#x3D;\frac{1}{2 m} \sum_{i&#x3D;1}^m \ell\left(h_\theta\left(x^{(i)}\right), y^{(i)}\right)&#x3D;\frac{1}{2 m} \sum_{i&#x3D;1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$</p>
</li>
<li><p>优化算法：给定训练集，如何找到最优的参数$\theta$使得<br>$$<br>\min _{\theta_0, \theta_1} J\left(\theta_0, \theta_1\right)<br>$$</p>
</li>
<li><p><strong>参数优化</strong></p>
<p>找到最优的参数$\theta^*&#x3D;arg~ \min_{\theta} J(\theta)$</p>
<ul>
<li>穷举所有$\theta$</li>
<li>随机搜索</li>
<li>梯度下降</li>
</ul>
</li>
</ul>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a><strong>梯度下降</strong></h4><p>  repeat until convergence{</p>
<p>  $\theta_j:&#x3D;\theta_j-\alpha \frac{\partial}{\partial \theta_j} J\left(\theta_0, \theta_1\right) \quad($ for $j&#x3D;0$ and $j&#x3D;1)$</p>
<p>  }</p>
<ul>
<li><p><strong>梯度</strong>：<br>$$<br>\nabla_\theta f(\theta) \in \mathbb{R}^n&#x3D;\left[\begin{array}{c}<br>\frac{\partial f(\theta)}{\partial \theta_1} \<br>\frac{\partial f(\theta)}{\partial \theta_2} \<br>\vdots \<br>\frac{\partial f(\theta)}{\partial \theta_n}<br>\end{array}\right]<br>$$<br>梯度下降算法的另一种表述：<br>$$<br>Repeat:~\theta&#x3D;\theta-\alpha \nabla_{\theta}f(\theta)<br>$$</p>
</li>
<li><p>单变量线性回归模型的梯度下降<br>$$<br>repeat<del>until</del>convergence:\<br>\theta_0&#x3D;\theta_0-\alpha \frac{1}{m} \sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)})\<br>\theta_1&#x3D;\theta_1-\alpha \frac{1}{m} \sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)}) \cdot x^{(i)}<br>$$</p>
</li>
</ul>
<h3 id="多特征（变量）"><a href="#多特征（变量）" class="headerlink" title="多特征（变量）"></a>多特征（变量）</h3><h4 id="三要素-1"><a href="#三要素-1" class="headerlink" title="三要素"></a>三要素</h4><ul>
<li>假设：$h_\theta(x)&#x3D;\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+\cdots+\theta_n x_n, x_0&#x3D;1$</li>
<li>参数：$\theta_0,\theta_1,\dots,\theta_n$</li>
<li>目标函数：$J\left(\theta_0, \theta_1, \cdots, \theta_n\right)&#x3D;\frac{1}{2 m} \sum_{i&#x3D;1}^m \ell\left(h_\theta\left(x^{(i)}\right), y^{(i)}\right)&#x3D;\frac{1}{2 m} \sum_{i&#x3D;1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$</li>
</ul>
<h4 id="梯度下降-1"><a href="#梯度下降-1" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>$$<br>repeat<del>until</del>convergence:\<br>\theta_j:&#x3D;\theta_j-\alpha \frac{\partial}{\partial \theta_j} J\left(\theta_0, \theta_1,\dots,\theta_n\right)~~j&#x3D;0,1,\dots,n<br>$$</p>
<h4 id="特征尺度归一化"><a href="#特征尺度归一化" class="headerlink" title="特征尺度归一化"></a>特征尺度归一化</h4><ul>
<li><p>范围归一化：使得每个特征尽量接近某个范围，如$0 \le x_i \le 1$</p>
</li>
<li><p>零均值归一化：用$x_i-\mu_i$替代$x_i$，即$x_i- \mu_i \to x_i$，其中$\mu_i&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{m} x_i$为均值</p>
</li>
<li><p>零均值+范围归一化</p>
</li>
<li><p>零均值单位方差归一化：<br>$$<br>\frac{x_i- \mu_i}{\sigma_i} \to x_i<br>$$</p>
</li>
</ul>
<h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p>梯度下降$\theta_j:&#x3D;\theta_j-\alpha \frac{\partial}{\partial \theta_j} J\left(\theta \right)$</p>
<p>收敛条件$\Delta J(\theta) \le 10^{-3}$</p>
<p>自动收敛测试</p>
<ul>
<li>对于足够小的$\alpha,J(\theta)$应该在每一次迭代中减小</li>
<li>如果$\alpha$太小，梯度下降算法收敛速度慢</li>
<li>反之，梯度下降算法不会收敛、发散或者震荡</li>
</ul>
<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><ul>
<li>对于求函数极小值问题，除了迭代方法之外，可以<strong>令函数的微分为零，然后求解方程</strong></li>
</ul>
<p>$$<br>\theta \in \mathbb{R}^{n+1},\<br>J\left(\theta_0, \theta_1, \cdots, \theta_n\right)&#x3D;\frac{1}{2 m} \sum_{i&#x3D;1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2\<br>\nabla_\theta J(\theta)&#x3D;0<br>$$</p>
<p>解出$\theta_0,\theta_1,\dots,\theta_n$</p>
<p>考虑到求和需要进行循环，使用矩阵运算会有更小的时间复杂度</p>
<p>得到$J(\theta)&#x3D;\frac{1}{2m}(X\theta-y)^T(X\theta-y)$</p>
<p>则$\nabla_\theta J(\theta)&#x3D;\frac{1}{m}(X^TX\theta-X^Ty)&#x3D;0$</p>
<p>解出$\theta&#x3D;(X^TX)^{-1}X^Ty$</p>
<h4 id="梯度下降和正规方程的比较"><a href="#梯度下降和正规方程的比较" class="headerlink" title="梯度下降和正规方程的比较"></a>梯度下降和正规方程的比较</h4><p>m训练样本，n个特征</p>
<ul>
<li>梯度下降<ul>
<li>需要选择合适的$\alpha$</li>
<li>需要多次迭代</li>
<li>即使n很大效果也很好</li>
</ul>
</li>
<li>正规方程<ul>
<li>不需要选择$\alpha$</li>
<li>不需要迭代</li>
<li>需要计算$(X^TX)^{-1}$</li>
<li>n很大会导致求解很慢</li>
<li>矩阵不可逆时需要删减一些特征，或者进行正则化</li>
</ul>
</li>
</ul>
<h2 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h2><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>目标：$0 \le h_\theta(x) \le 1$<br>$$<br>h_\theta(x)&#x3D;g(\theta^Tx)&#x3D;\frac{1}{1+e^{-\theta^Tx}}<br>$$</p>
<h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p><img src="/imgs/ml/2.png" alt="image-20230927092917418"></p>
<h4 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h4><p>$$<br>h_\theta(x)&#x3D;g(\theta^Tx)&#x3D;\frac{1}{1+e^{-\theta^Tx}}<br>$$</p>
<p>$h_\theta(x)$对于输入x，输出y&#x3D;1的可能性</p>
<p>给出x，估计y&#x3D;1的可能性，$\theta$为参数<br>$$<br>P(y&#x3D;0|x;\theta)+P(y&#x3D;1|x;\theta)&#x3D;1<br>$$</p>
<h4 id="分类边界"><a href="#分类边界" class="headerlink" title="分类边界"></a>分类边界</h4><p>数形结合可知，$h_{\theta}(x)$形成的一个闭合曲线&#x2F;曲面作为一个分类边界<br>$$<br>h_{\theta}(x) \ge c \to y&#x3D;1<br>$$</p>
<p>$$<br>h_{\theta}(x) \lt c \to y&#x3D;0<br>$$</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>$$<br>P(y&#x3D;0|x;\theta)&#x3D;h_{\theta}(x)<br>$$</p>
<p>$$<br>P(y&#x3D;1|x;\theta)&#x3D;1-h_{\theta}(x)<br>$$</p>
<p>$$<br>p(y|x;\theta)&#x3D;(h_\theta(x))^y(1-h_\theta(x))^{1-y}<br>$$</p>
<p>$$<br>L(\theta)&#x3D;p(y|\mathbf{X};\theta)&#x3D;\prod_{i&#x3D;1}^m p(y^{(i)}|x^{(i)};\theta)&#x3D;\prod_{i&#x3D;1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}<br>$$</p>
<p><strong>Logistic损失函数</strong><br>$$<br>\ell(\theta)&#x3D;-\log L(\theta)&#x3D;-\left[\sum_{i&#x3D;1}^m y^{(i)} \log h_\theta(x^{(i)})+(1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right]<br>$$<br><strong>cross entropy 交叉熵</strong><br>$$<br>H(p,q)&#x3D;-\sum_x p(x) \log q(x)<br>$$<br>通俗来说，$p(x)$是真实样本的分布，$q(x)$是预测样本的分布，cross entropy代表的是两个分布之间的距离，越小说明分布越接近，迭代过程中需要将ce降下来</p>
<p><strong>分类问题中常用</strong><br>$$<br>\operatorname{Cost}\left(h_\theta(x), y\right)&#x3D;\left{\begin{aligned}<br>-\log \left(h_\theta(x)\right) &amp; \text { if } y&#x3D;1 \<br>-\log \left(1-h_\theta(x)\right) &amp; \text { if } y&#x3D;0<br>\end{aligned}\right.<br>$$</p>
<h4 id="梯度下降-2"><a href="#梯度下降-2" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>$$<br>J(\theta)&#x3D;-\left[\sum_{i&#x3D;1}^m y^{(i)} \log h_\theta(x^{(i)})+(1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right]<br>$$</p>
<p>找到合适的参数$\theta$使得$\min_\theta J(\theta)$</p>
<p>repeat{<br>$$<br>\theta_j&#x3D;\theta_j-\alpha \frac{\partial}{\partial \theta_j}J(\theta)<br>$$<br>}</p>
<p>$\frac{\partial}{\partial \theta_j}J(\theta)&#x3D;(h_\theta(x)-y)x_j$</p>
<p>不可以直接使用线性回归中的平方损失函数</p>
<p><img src="/imgs/ml/3.png" alt="image-20231008163324478"></p>
<p>黑色的是cross entropy，红色的是square error</p>
<h4 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h4><p>$$<br>h_{\theta_i}&#x3D;P(y&#x3D;i|x;\theta_i),i&#x3D;1,2,3,…<br>$$</p>
<p>一对多</p>
<ul>
<li><p>为每类训练一个逻辑回归分类器$h_{\theta_i}(x)$用来预测$y&#x3D;i$的可能性</p>
</li>
<li><p>对于新输入$x$，做一个预测，选择一个类别$i^*$使得：<br>$$<br>i^*&#x3D;arg \max_i h_{\theta_i}(x)<br>$$</p>
</li>
</ul>
<h4 id="softmax-Regression"><a href="#softmax-Regression" class="headerlink" title="softmax Regression"></a>softmax Regression</h4><p>$$<br>p(y&#x3D;i|x;\theta)&#x3D;h_{\theta_i}(x)&#x3D;\frac{e^{z_i}}{\sum_{j&#x3D;1}^K e^{z_j}},z_j&#x3D;(\theta_i)^Tx<br>$$</p>
<p>对数似然为<br>$$<br>L(\theta)&#x3D;\sum_{i&#x3D;1}^{m} \log p(y^{(i)}|x^{(i)};\theta)&#x3D;\sum_{i&#x3D;1}^{m} \log (\frac{e^zy^{(i)}}{\sum_{j&#x3D;1}^K e^{z_j}})<br>$$<br>总损失为<br>$$<br>\ell(\theta)&#x3D;-L(\theta)&#x3D;-\sum_{i&#x3D;1}^{m} \log (\frac{e^zy^{(i)}}{\sum_{j&#x3D;1}^K e^{z_j}})&#x3D;\sum_{i&#x3D;1}^m \left[\log(\sum_{j&#x3D;1}^K e^{z_j})-z_{y^(i)}  \right]<br>$$</p>
<p>$$<br>\hat{y_i}&#x3D;h_{\theta_i}(x)<br>$$</p>
<h2 id="模型选择与正则化"><a href="#模型选择与正则化" class="headerlink" title="模型选择与正则化"></a>模型选择与正则化</h2><h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p>偏差bias，方差variance<br>$$<br>bias(h(x))&#x3D;E[h(x)-y(x)]<br>$$</p>
<p>$$<br>var(h(x))&#x3D;E \lbrace h(x)-E[h(x)] \rbrace<br>$$</p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果多项式阶数较大，训练得到的模型对于训练集能正确拟合$J(\theta)&#x3D;\frac{1}{2m}[h_\theta(x^{(i)}-y^{(i)})^2] \approx  0$，但是对于新的样本预测效果却不好</p>
<p>实际应用中容易出现过拟合</p>
<p>绘制这个模型的学习曲线</p>
<p>通过学习曲线的形态来判断</p>
<p>所谓学习曲线就是训练集得分和验证集得分随着训练样本数的增大而变化的曲线</p>
<p><strong>欠拟合情况</strong>：随着训练样本数增大，训练集得分和验证集得分收敛，并且两者的收敛值很接近</p>
<p><strong>过拟合情况</strong>：随着训练样本数增大，训练集得分和验证集得分相差还是很大</p>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>将训练集随机分成两部分：用于训练参数的训练集和用于模型选择的验证集</p>
<p><img src="/imgs/ml/4.png" alt="image-20231015142857582"></p>
<p><img src="/imgs/ml/5.png" alt="image-20231015142932617"></p>
<p><img src="/imgs/ml/6.png" alt="image-20231015142956584"></p>
<h3 id="诊断误差和方差"><a href="#诊断误差和方差" class="headerlink" title="诊断误差和方差"></a>诊断误差和方差</h3><p>训练误差：$L_{train}(\theta)&#x3D;\frac{1}{2m} \sum_{i&#x3D;1}^m (h_\theta(x^{(i)})-y^{(i)})^2$</p>
<p>验证误差：$L_{val}(\theta)&#x3D;\frac{1}{2m_{val}} \sum_{i&#x3D;1}^{m_{val}} (h_\theta(x_{val}^{(i)})-y_{val}^{(i)})^2$</p>
<h4 id="偏差大（underfit欠拟合）"><a href="#偏差大（underfit欠拟合）" class="headerlink" title="偏差大（underfit欠拟合）"></a>偏差大（underfit欠拟合）</h4><p>训练误差：大</p>
<p>训练误差与验证误差差别较小</p>
<h4 id="方差大（overfit过拟合）"><a href="#方差大（overfit过拟合）" class="headerlink" title="方差大（overfit过拟合）"></a>方差大（overfit过拟合）</h4><p>训练误差：小</p>
<p>验证误差远大于训练误差</p>
<p><img src="/imgs/ml/7.png" alt="image-20231015143732091"></p>
<p>从图像上可以知道，靠左侧是欠拟合，靠右侧是过拟合</p>
<h3 id="解决欠拟合和过拟合问题"><a href="#解决欠拟合和过拟合问题" class="headerlink" title="解决欠拟合和过拟合问题"></a>解决欠拟合和过拟合问题</h3><h4 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h4><p>核心：增加模型的复杂度</p>
<ul>
<li>收集新的特征</li>
<li>增加多项式组合特征</li>
<li>…</li>
</ul>
<h4 id="过拟合-1"><a href="#过拟合-1" class="headerlink" title="过拟合"></a>过拟合</h4><ul>
<li>增加数据</li>
<li>降低模型的复杂度<ul>
<li>减少特征（人为筛选）</li>
<li>正则化，可降低方差提高偏差</li>
</ul>
</li>
</ul>
<h3 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h3><h4 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h4><p>$$<br>\min_\theta J(\theta)<br>$$</p>
<p>$$<br>J(\theta)&#x3D;\frac{1}{2m} \left[ \sum_{i&#x3D;1}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum_{j&#x3D;1}^n \theta_j^2 \right]<br>$$</p>
<p>$$<br>L(\theta)&#x3D;\frac{1}{2m} \sum_{i&#x3D;1}^m (h_\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
<p>$$<br>J(\theta)&#x3D;L(\theta)+\lambda R(\theta)<br>$$</p>
<p>gradient descent</p>
<p>repeat{<br>$$<br>\theta_j&#x3D;\theta_j(1-\alpha \frac{\lambda}{m})-\alpha \frac{1}{m} \sum_{i&#x3D;1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$<br>}</p>
<p><strong>正则化参数$\lambda$的选择</strong><br>$$<br>J(\theta)&#x3D;\frac{1}{2m} \left[ \sum_{i&#x3D;1}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum_{j&#x3D;1}^n \theta_j^2 \right]<br>$$</p>
<p>$$<br>L(\theta)&#x3D;\frac{1}{2m} \sum_{i&#x3D;1}^m (h_\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
<h4 id="RegularizedNormal-equation"><a href="#RegularizedNormal-equation" class="headerlink" title="RegularizedNormal equation"></a>RegularizedNormal equation</h4><p>$$<br>\theta&#x3D;\left(X^T X+\lambda\left[\begin{array}{lllll}<br>0 &amp; &amp; &amp; &amp; \<br>&amp; 1 &amp; &amp; &amp; \<br>&amp; &amp; 1 &amp; &amp; \<br>&amp; &amp; &amp; \ddots &amp; \<br>&amp; &amp; &amp; &amp; 1<br>\end{array}\right]\right) X^{-1} X^T y<br>$$</p>
<h3 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h3><p>$$<br>J(\theta)&#x3D;\left[-\frac{1}{m} \sum_{i&#x3D;1}^m y^{(i)} \log \left(h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log (1-h_\theta\left(x^{(i)}\right))\right]+\frac{\lambda}{2 m} \sum_{j&#x3D;1}^n \theta_j^2\right.<br>$$</p>
<p>梯度下降同上</p>
<p>$$<br>J(\theta)&#x3D;L(\theta)+\lambda R(\theta)<br>$$</p>
<p>$$<br>L(\theta)&#x3D;\left[-\frac{1}{m} \sum_{i&#x3D;1}^m y^{(i)} \log (h_\theta(x^{(i)})+(1-y^{(i)}) \log (1-h_\theta(x^{(i)}))\right]<br>$$</p>
<h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p><img src="/imgs/ml/8.png" alt="image-20231016190009923"></p>
<p>如果一个模型测试结果是high bias，使用更多的训练数据并不能改进模型</p>
<p><img src="/imgs/ml/9.png" alt="image-20231016190147599"></p>
<p>如果一个模型测试结果是high variance，使用更多的训练数据会有效改进模型</p>
<h3 id="模型性能评估"><a href="#模型性能评估" class="headerlink" title="模型性能评估"></a>模型性能评估</h3><ul>
<li>用训练集训练参数</li>
</ul>
<p>$$<br>\theta^*&#x3D;a\arg \min_\theta \frac{1}{m} \sum_{i&#x3D;1}^m \ell(h_\theta(x^{(i)}),y^{(i)})<br>$$</p>
<ul>
<li>用验证集选择模型，用于调参（正则化参数、多项式阶数、特征选择）</li>
<li>测试集仅用于性能评估</li>
</ul>
<h4 id="验证集和测试集"><a href="#验证集和测试集" class="headerlink" title="验证集和测试集"></a>验证集和测试集</h4><ul>
<li>验证集和测试集应具有同分布</li>
<li>验证集和测试集的大小<ul>
<li>验证集：1000-10000；应当足够大</li>
<li>测试集：中小30%；大数据足够大</li>
</ul>
</li>
</ul>
<h4 id="交叉验证-k-fold-cross-validation"><a href="#交叉验证-k-fold-cross-validation" class="headerlink" title="交叉验证 k-fold cross validation"></a>交叉验证 k-fold cross validation</h4><ul>
<li>数据集规模较小情况下采用</li>
<li>把数据随机划分为k等份，每次用其中的(k - 1)份做训练，剩下的做验证</li>
<li>计算平均误差（和方差）</li>
</ul>
<h2 id="神经网络与深度学习"><a href="#神经网络与深度学习" class="headerlink" title="神经网络与深度学习"></a>神经网络与深度学习</h2><h3 id="非线性分类"><a href="#非线性分类" class="headerlink" title="非线性分类"></a>非线性分类</h3><h3 id="The-“one-learning-algorithm”-hypothesis"><a href="#The-“one-learning-algorithm”-hypothesis" class="headerlink" title="The “one learning algorithm” hypothesis"></a>The “one learning algorithm” hypothesis</h3><h3 id="神经元模型：Logistic-unit"><a href="#神经元模型：Logistic-unit" class="headerlink" title="神经元模型：Logistic unit"></a>神经元模型：Logistic unit</h3><p><img src="/imgs/ml/10.png" alt="image-20231016192226609"></p>
<h3 id="全连接前馈网络-Fully-Connect-Feedforward-Network"><a href="#全连接前馈网络-Fully-Connect-Feedforward-Network" class="headerlink" title="全连接前馈网络 Fully Connect Feedforward Network"></a>全连接前馈网络 Fully Connect Feedforward Network</h3><p><img src="/imgs/ml/11.png" alt="image-20231016192336825"></p>
<p><img src="/imgs/ml/12.png" alt="image-20231016192436942"></p>
<p>如果在第j层有$s_j$个units，第j+1层有$s_{j+1}$个units，则$\theta^{(j)}$的维数是$s_j \times s_{j+1}$</p>
<p>FCFN的执行过程</p>
<p>激活函数$g(z)&#x3D;\frac{1}{1+e^{-z}}$</p>
<p><img src="/imgs/ml/13.png" alt="image-20231016193208513"></p>
<h3 id="前向传播：矩阵表示"><a href="#前向传播：矩阵表示" class="headerlink" title="前向传播：矩阵表示"></a>前向传播：矩阵表示</h3><p><img src="/imgs/ml/14.png" alt="image-20231016193446007"><br>$$<br>h_\theta(x)&#x3D;g(\theta^{(2)}g(\theta^{(1)}x))<br>$$</p>
<h3 id="特征学习"><a href="#特征学习" class="headerlink" title="特征学习"></a>特征学习</h3><p><img src="/imgs/ml/15.png" alt="image-20231016193745129"></p>
<h3 id="多层神经网络"><a href="#多层神经网络" class="headerlink" title="多层神经网络"></a>多层神经网络</h3><p><img src="/imgs/ml/16.png" alt="image-20231016193817513"></p>
<h4 id="用神经网络求解XOR-x2F-XNOR问题"><a href="#用神经网络求解XOR-x2F-XNOR问题" class="headerlink" title="用神经网络求解XOR&#x2F;XNOR问题"></a>用神经网络求解XOR&#x2F;XNOR问题</h4><p><img src="/imgs/ml/18.png" alt="image-20231017090005632"></p>
<p><img src="/imgs/ml/17.png" alt="image-20231017085931923"></p>
<p><img src="/imgs/ml/19.png" alt="image-20231017090721937"></p>
<p><img src="/imgs/ml/20.png" alt="image-20231017090803876"></p>
<h4 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h4><p><img src="/imgs/ml/21.png" alt="image-20231017091225042"></p>
<h4 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h4><p><img src="/imgs/ml/22.png" alt="image-20231017091338145"></p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>$$<br>h_\theta(x)&#x3D;g_L(\theta^{L-1}g_{L-1}(\dots g_2(\theta^{(2)}g_1(\theta^{(1)}x))))<br>$$</p>
<h3 id="DeepLearning-Many-hidden-layers"><a href="#DeepLearning-Many-hidden-layers" class="headerlink" title="DeepLearning:Many hidden layers"></a>DeepLearning:Many hidden layers</h3><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>$$<br>z&#x3D;\sum \theta_ix_i +\theta_0<br>$$</p>
<p>$$<br>\frac{dz}{d\theta_i}&#x3D;x_i<br>$$</p>
<p>$$<br>\frac{dJ}{d\theta_i}&#x3D;\frac{dJ}{dz} \frac{dz}{d \theta_i}&#x3D;\frac{dJ}{dz}x_i<br>$$</p>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p><img src="/imgs/ml/23.png" alt="image-20231020160853644"><br>$$<br>sigmoid:g(x)&#x3D;\frac{1}{1+e^{-x}}<br>$$</p>
<ol>
<li>Saturated neurons “kill” the gradients</li>
<li>Sigmoid outputs are not zerocentered</li>
<li>exp() is a bit compute expensive</li>
</ol>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p><img src="/imgs/ml/24.png" alt="image-20231020161036525"><br>$$<br>tanh:g(x)&#x3D;\frac{e^x-e^{-x}}{e^x+e^{-x}}<br>$$</p>
<ol>
<li><p>Zero centered (nice)</p>
</li>
<li><p>“kill”the gradients</p>
</li>
<li><p>exp() is a bit compute expensive</p>
</li>
</ol>
<h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><img src="/imgs/ml/25.png" alt="image-20231020161629731"><br>$$<br>ReLU:g(x)&#x3D;\max(0,x)<br>$$</p>
<ol>
<li><p>Does not saturate (in +region)</p>
</li>
<li><p>Very computationally efficient</p>
</li>
<li><p>Converges much faster than sigmoid&#x2F;tanh in practice (e.g. 6x)</p>
</li>
<li><p>Actually more biologically plausible than sigmoid</p>
</li>
<li><p>Not zero-centered output</p>
</li>
<li><p>x&lt;0: dead ReLU will never activate &#x3D;&gt; never update</p>
</li>
</ol>
<h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><p><img src="/imgs/ml/26.png" alt="image-20231020161851458"><br>$$<br>Leaky ReLU:g(x)&#x3D;\max(0.1x,x)<br>$$</p>
<ol>
<li><p>Does not saturate (in +region)</p>
</li>
<li><p>Very computationally efficient</p>
</li>
<li><p>Converges much faster than sigmoid&#x2F;tanh in practice (e.g. 6x)</p>
</li>
<li><p>Actually more biologically plausible than sigmoid</p>
</li>
<li><p>Not zero-centered output</p>
</li>
<li><p>Will not “die”</p>
</li>
</ol>
<p>$$<br>Parametric\ Rectifier\ Linear\ Unit(PReLU):g(x)&#x3D;\max(\alpha x,x)<br>$$</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>$$<br>l(y,\hat{y})&#x3D;-\sum_{i&#x3D;1}^n y_i \log(\hat{y_i})<br>$$</p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>$$<br>J(\theta)&#x3D;L(\theta)+\lambda R(\theta)<br>$$</p>
<p>$$<br>h_\theta(x) \in \mathbb{R}^K,(h_\theta(x))_k&#x3D;k^{th}\ output<br>$$</p>
<p>$$<br>\begin{aligned}<br>J(\Theta) &amp; &#x3D;-\frac{1}{m}\left[\sum_{i&#x3D;1}^m \log p\left(y^{(i)} \mid x^{(i)} ; \Theta\right)\right]+\frac{\lambda}{2 m} \sum_{l&#x3D;1}^{L-1} \sum_{i&#x3D;1}^{s_l} \sum_{j&#x3D;1}^{s_{l+1}}\left(\Theta_{j i}^{(l)}\right)^2 \<br>&amp; &#x3D;-\frac{1}{m} \sum_{i&#x3D;1}^m \sum_{k&#x3D;1}^K y_k^{(i)} \log \left(h_{\Theta}\left(x^{(i)}\right)\right)<em>k+\frac{\lambda}{2 m} \sum</em>{l&#x3D;1}^{L-1} \sum_{i&#x3D;1}^{s_l} \sum_{j&#x3D;1}^{s_{l+1}}\left(\Theta_{j i}^{(l)}\right)^2<br>\end{aligned}<br>$$</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>$$<br>\min_\theta J(\theta)<br>$$</p>
<h3 id="梯度计算：反向传播"><a href="#梯度计算：反向传播" class="headerlink" title="梯度计算：反向传播"></a>梯度计算：反向传播</h3><p>BackPropagation，BP</p>
<h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p><img src="/imgs/ml/27.png" alt="image-20231020170049734"></p>
<h4 id="梯度矢量化表示"><a href="#梯度矢量化表示" class="headerlink" title="梯度矢量化表示"></a>梯度矢量化表示</h4><p><img src="/imgs/ml/28.png" alt="image-20231020170507628"></p>
<h4 id="Batch-gradient-descent-vs-Stochastic-gradient-descent"><a href="#Batch-gradient-descent-vs-Stochastic-gradient-descent" class="headerlink" title="Batch gradient descent vs. Stochastic gradient descent"></a>Batch gradient descent vs. Stochastic gradient descent</h4><p><img src="/imgs/ml/29.png" alt="image-20231020171456260"></p>
<p><img src="/imgs/ml/30.png" alt="image-20231020171603203"></p>
<p><strong>Mini-batch gradient descent</strong></p>
<p><img src="/imgs/ml/31.png" alt="image-20231020171644520"></p>
<ul>
<li><p>Batch gradient descent</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nb_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
	params_grad<span class="token operator">=</span>evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span>data<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
	params<span class="token operator">=</span>params<span class="token operator">-</span>learning_rate<span class="token operator">*</span>params_grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>Stochastic gradient descent</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nb_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
	np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
	<span class="token keyword">for</span> example <span class="token keyword">in</span> data<span class="token punctuation">:</span>
		params_grad<span class="token operator">=</span>evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span>example<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
		params<span class="token operator">=</span>params<span class="token operator">-</span>learning_rate<span class="token operator">*</span>params_grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>Mini-batch gradient descent</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nb_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
	np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
	<span class="token keyword">for</span> batch <span class="token keyword">in</span> get_batches<span class="token punctuation">(</span>data<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		params_grad<span class="token operator">=</span>evaluate_gradient<span class="token punctuation">(</span>loss_function<span class="token punctuation">,</span>batch<span class="token punctuation">,</span>params<span class="token punctuation">)</span>
		params<span class="token operator">=</span>params<span class="token operator">-</span>learning_rate<span class="token operator">*</span>params_grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<h3 id="Stepsize-vs-Gradient"><a href="#Stepsize-vs-Gradient" class="headerlink" title="Stepsize vs. Gradient"></a>Stepsize vs. Gradient</h3><p><img src="/imgs/ml/32.png" alt="image-20231020172336633"></p>
<h3 id="Adaptive-学习率"><a href="#Adaptive-学习率" class="headerlink" title="Adaptive 学习率"></a>Adaptive 学习率</h3><p><img src="/imgs/ml/33.png" alt="image-20231020172436170"></p>
<h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>$$<br>\alpha^t&#x3D;\frac{\alpha}{\sqrt{t+1}},g_t&#x3D;\frac{\partial J(\theta^t)}{\partial \theta}<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-\frac{\alpha^t}{\sigma^t}g_t,\sigma^t&#x3D;\sqrt{\frac{1}{t+1}\sum_{i&#x3D;0}^{t}g_i^2}<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-\frac{\alpha^t}{\sqrt{\sum_{i&#x3D;0}^tg_i^2}}g_t<br>$$</p>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>$$<br>u^0&#x3D;0<br>$$</p>
<p>$$<br>u^t&#x3D;\rho u^{t-1}+(1-\rho)g_t^2<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-\frac{\alpha}{\sqrt{u^t}}g_t<br>$$</p>
<h4 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h4><p><img src="/imgs/ml/34.png" alt="image-20231021145518518"></p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p><strong>GD+Momentum</strong><br>$$<br>v^0&#x3D;0<br>$$</p>
<p>$$<br>v^t&#x3D;\rho v^{t-1}+\alpha g_t<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-v^t<br>$$</p>
<p>引入速度（前面梯度的加权均值）</p>
<p><img src="/imgs/ml/35.png" alt="image-20231021145743467"></p>
<p><strong>Nesterov Momentum</strong></p>
<p><img src="/imgs/ml/36.png" alt="image-20231021145908869"><br>$$<br>v^t&#x3D;\rho v^{t-1}+\alpha \frac{\partial J(\theta^t+\rho v^{t-1})}{\partial \theta}<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-v^t<br>$$</p>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>$$<br>m^0&#x3D;0,v^0&#x3D;0<br>$$</p>
<p>$$<br>m^t&#x3D;\beta_1 m^{t-1}+(1-\beta_1)g_t<br>$$</p>
<p>$$<br>v^t&#x3D;\beta_2 v^{t-1}+(1-\beta_2)g_t^2<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-\frac{\alpha}{\sqrt{v^t+\epsilon}}m%t<br>$$</p>
<p><img src="/imgs/ml/37.png" alt="image-20231021161930422"></p>
<h4 id="AdaBelief"><a href="#AdaBelief" class="headerlink" title="AdaBelief"></a>AdaBelief</h4><p><img src="/imgs/ml/38.png" alt="image-20231021162335145"><br>$$<br>s^t&#x3D;\beta_2 s^{t-1}+(1-\beta_2)(g_t-m_t)^2<br>$$</p>
<p>$$<br>\theta^{t+1}&#x3D;\theta^t-\frac{\alpha}{\sqrt{s^t+\epsilon}}m%t<br>$$</p>
<p>Adam or AdaBelief might be the best overall choice</p>
<p>SGD</p>
<ul>
<li>usually achieves to find a minimum, but it might take significantly longer</li>
<li>is much more reliant on a robust initialization and annealing schedule</li>
<li>may get stuck in saddle points rather than local minima</li>
</ul>
<h3 id="Additional-strategies-for-optimizing-SGD"><a href="#Additional-strategies-for-optimizing-SGD" class="headerlink" title="Additional strategies for optimizing SGD"></a>Additional strategies for optimizing SGD</h3><ul>
<li>Shuffling and Curriculum Learning 打乱数据集和课程学习</li>
<li>Early stopping: Early stopping (is) beautiful free lunch (Geoff Hinton) 早停</li>
<li>Gradient noise: add noise to each gradient update 梯度噪声</li>
</ul>
<h3 id="二阶方法：牛顿法"><a href="#二阶方法：牛顿法" class="headerlink" title="二阶方法：牛顿法"></a>二阶方法：牛顿法</h3><p>迭代规则<br>$$<br>\theta:&#x3D;\theta-\frac{f(\theta)}{f’(\theta)}<br>$$</p>
<p>$$<br>\theta:&#x3D;\theta-\frac{f(\theta)}{f’(\theta)},\min_\theta J(\theta) \to J’(\theta)&#x3D;0,f(\theta)&#x3D;J’(\theta)<br>$$</p>
<p>$$<br>\theta&#x3D;\theta-\frac{J’(\theta)}{J’’(\theta)}<br>$$</p>
<p>$\theta$是矢量，则$\theta:&#x3D;\theta-H^{-1} \nabla_\theta J(\theta) ,H_{ij}&#x3D;\frac{\partial^2 \ell(\theta)}{\partial \theta_i \partial \theta_j}$</p>
<p>由于二阶方法需要计算Hessian矩阵，在参数数目较多的情况下计算复杂，目前主要仍采用一阶方法</p>
<h3 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a>Gradient checking</h3><p>$$<br>\frac{\partial J(\theta)}{\partial \theta} \approx \frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}<br>$$</p>
<p><img src="/imgs/ml/39.png" alt="image-20231021165426644"></p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>不能zero initialization，否则训练出全部相同的神经元</p>
<p>Random initialization (Gaussian with zero mean and 1e-2 standard deviation)，Works ~okay for small networks, but problems with deeper networks</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p><img src="/imgs/ml/40.png" alt="image-20231021165926172"></p>
<h3 id="网格结构选择与设计"><a href="#网格结构选择与设计" class="headerlink" title="网格结构选择与设计"></a>网格结构选择与设计</h3><p>前一层的输出与后一层的输入需要一致</p>
<h3 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h3><ul>
<li>Initialize weights</li>
<li>Implement forward propagation</li>
<li>Implement code to compute cost function</li>
<li>Implement backpropagation code to compute gradient</li>
<li>Implement numerical gradient check (disable your gradient checking code before training)</li>
<li>Double check that the loss is reasonable</li>
<li>Make sure that you can overfit very small portion of the training data</li>
<li>Start with small regularization and find learning rate that makes the loss go down</li>
</ul>
<h2 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h2><p>$$<br>\hat{\gamma}^{(i)}&#x3D;y^{(i)}(w^Tx^{(i)}+b)<br>$$</p>
<p>$$<br>y&#x3D;\operatorname{sign}\left(w^T x+b\right)&#x3D; \begin{cases}+1, &amp; w^T x+b&gt;0 \ -1, &amp; w^T x+b&lt;0\end{cases}<br>$$</p>
<p>要求：<br>$$<br>y^{(i)}(w^Tx^{(i)}+b)&gt;0<br>$$</p>
<h3 id="最大间隔分类器-Max-Margin-Classifier"><a href="#最大间隔分类器-Max-Margin-Classifier" class="headerlink" title="最大间隔分类器(Max Margin Classifier)"></a>最大间隔分类器(Max Margin Classifier)</h3><p><img src="/imgs/ml/41.png" alt="image-20231025082744416"><br>$$<br>w^Tx^++b&#x3D;+1,w^Tx^-+b&#x3D;-1,x^+&#x3D;x^-+\lambda w<br>$$</p>
<p>$$<br>margin&#x3D;\parallel x^+-x^- \parallel<br>$$</p>
<p>$$<br>w^T(x^+-x^-)&#x3D;2<br>$$</p>
<p>$$<br>\lambda&#x3D;\frac{2}{w^Tw}<br>$$</p>
<p>$$<br>margin&#x3D;\parallel x^+-x^-\parallel &#x3D;\parallel \lambda w \parallel&#x3D;\frac{2}{\parallel w \parallel}<br>$$</p>
<p>$$<br>margin:\max_w \frac{2}{\parallel w \parallel} \to \min_w \frac{1}{2}\parallel w \parallel^2<br>$$</p>
<h3 id="The-Primal-Hard-SVM"><a href="#The-Primal-Hard-SVM" class="headerlink" title="The Primal Hard SVM"></a>The Primal Hard SVM</h3><p>假设数据线性可分，即$y^{(i)}(w^Tx^{(i)}+b) \ge 1$</p>
<p><em>关于函数间隔为什么可设置为1，参考<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/64568136">机器学习SVM中关于函数间隔为什么可以设置为1？</a>，这里面写的比较详细</em><br>$$<br>\begin{aligned}<br>\min _{w, b} &amp; \frac{1}{2}|w|^2 \<br>\text { s.t. } &amp; y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1, \quad i&#x3D;1, \ldots, m<br>\end{aligned}<br>$$<br>典型的二次规划问题</p>
<h4 id="Equality-constraint"><a href="#Equality-constraint" class="headerlink" title="Equality constraint"></a>Equality constraint</h4><p>$$<br>\max_x f(x)<br>$$</p>
<p>$$<br>\text { s.t. }  g(x)&#x3D;0<br>$$</p>
<p>$$<br>\nabla f(x^*,y^*)+\lambda \nabla g(x^*,y^*)&#x3D;0,\beta \neq 0<br>$$</p>
<p>定义Lagrange函数$\mathcal L(x,y,\lambda)&#x3D;f(x,y)+\lambda g(x,y)$</p>
<p>令$\nabla_{x,y} \mathcal L&#x3D;0$得到上式</p>
<p>令$\nabla_{\lambda} \mathcal L&#x3D;0$得到约束条件$g(x,y)&#x3D;0$</p>
<p><img src="/imgs/ml/42.png" alt="image-20231025091620975"></p>
<h4 id="Inequality-constraint"><a href="#Inequality-constraint" class="headerlink" title="Inequality constraint"></a>Inequality constraint</h4><p>$$<br>\max_x f(\mathbf x)<br>$$</p>
<p>$$<br>\text{s.t.} g(\mathbf x) \ge 0<br>$$</p>
<p>$$<br>\mathcal{L}(\mathbf x,\lambda)&#x3D;f(\mathbf x)+\lambda g(\mathbf x)<br>$$</p>
<p>无效：</p>
<p>最优解本身满足约束$g(\mathbf x) &gt;0$，约束条件不起作用，即<br>$$<br>\lambda&#x3D;0<br>$$<br>有效：</p>
<p>最优解再约束条件边界上，$g(\mathbf x) &#x3D;0$，此时$\nabla g(\mathbf x)$方向必定与$\nabla f(\mathbf x)$相反，即<br>$$<br>\nabla f(\mathbf x)+\lambda \nabla g(\mathbf x)&#x3D;0,\lambda &gt;0<br>$$<br>两种情况都有$\lambda g(\mathbf x)&#x3D;0$</p>
<h4 id="Lagrange-Multiplier"><a href="#Lagrange-Multiplier" class="headerlink" title="Lagrange Multiplier"></a>Lagrange Multiplier</h4><h5 id="equality"><a href="#equality" class="headerlink" title="equality"></a>equality</h5><p>$$<br>\min_w f(w)<br>$$</p>
<p>$$<br>\text{s.t.} h_i(w)&#x3D;0,i&#x3D;1,\dots,l<br>$$</p>
<p>$$<br>\mathcal{L} (w,\beta)&#x3D;f(w)+\sum_{i&#x3D;1}^l \beta_ih_i(w),\beta_i:\mathbf{Lagrange\ multipliers}<br>$$</p>
<p>$$<br>\frac{\partial \mathcal L}{\partial w}&#x3D;0;\frac{\partial \mathcal L}{\partial \beta_i}&#x3D;0<br>$$</p>
<h5 id="inequality-amp-equality"><a href="#inequality-amp-equality" class="headerlink" title="inequality&amp;equality"></a>inequality&amp;equality</h5><p>$$<br>\min_w f(w)<br>$$</p>
<p>$$<br>\text{s.t.}\ g_i(w) \le 0,i&#x3D;1,\dots,k<br>$$</p>
<p>$$<br>\ \ \ \ \ h_i(w)&#x3D;0,i&#x3D;1,\dots,l<br>$$</p>
<p>$$<br>\mathcal L(w,\alpha,\beta)&#x3D;f(w)+\sum_{i&#x3D;1}^k \alpha_i g_i(w)+\sum_{i&#x3D;1}^l \beta_i g_i(w)<br>$$</p>
<p>$$<br>\text{s.t.}\ \alpha_i g_i(w)&#x3D;0,i&#x3D;1,\dots,k<br>$$</p>
<p>$$<br>\ \ \ \alpha_i \ge 0,i&#x3D;1,\dots,k<br>$$</p>
<p>$$<br>\theta_{\mathcal{P}}(w)&#x3D;\max _{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)<br>$$</p>
<p>给出$w$，如果$w$违反了最初的约束(e.g. $g_i(w)&lt;0\ or \ h_i(w) \neq 0 \ for \ some \ i$)，应当能给出<br>$$<br>\theta_{\mathcal{P}}(w)&#x3D;\max <em>{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)&#x3D;\max <em>{\alpha, \beta: \alpha_i \geq 0}  f(w)+\sum</em>{i&#x3D;1}^k \alpha_i g_i(w)+\sum</em>{i&#x3D;1}^l \beta_i g_i(w)&#x3D;\infin<br>$$</p>
<h4 id="Primal-vs-Dual"><a href="#Primal-vs-Dual" class="headerlink" title="Primal vs. Dual"></a>Primal vs. Dual</h4><h5 id="Primal"><a href="#Primal" class="headerlink" title="Primal"></a>Primal</h5><p>$$<br>\theta_{\mathcal{P}}(w)&#x3D;\max <em>{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta),\mathcal L(w,\alpha,\beta)&#x3D;f(w)+\sum</em>{i&#x3D;1}^k \alpha_i g_i(w)+\sum_{i&#x3D;1}^l \beta_i g_i(w)<br>$$</p>
<p>$$<br>\theta_{\mathcal{P}}(w)&#x3D; \begin{cases}f(w) &amp; \text { if } w \text { satisfies primal constraints } \ \infty &amp; \text { otherwise. }\end{cases}<br>$$</p>
<p>$$<br>\min_w \theta_{\mathcal{P}}(w)&#x3D;\min_w \max _{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)<br>$$</p>
<h5 id="Dual"><a href="#Dual" class="headerlink" title="Dual"></a>Dual</h5><p>$$<br>\theta_{\mathcal D}(\alpha,\beta)&#x3D;\min_w \mathcal{L}(w, \alpha, \beta)<br>$$</p>
<p>$$<br>\max _{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta)<br>$$</p>
<h5 id="弱对偶性"><a href="#弱对偶性" class="headerlink" title="弱对偶性"></a>弱对偶性</h5><p>$$<br>d^*&#x3D;\max _{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta) \le \min_w \max _{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)&#x3D;p^*<br>$$</p>
<h5 id="强对偶性"><a href="#强对偶性" class="headerlink" title="强对偶性"></a>强对偶性</h5><p>$$<br>d^*&#x3D;\max _{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta) &#x3D; \min_w \max _{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)&#x3D;p^*<br>$$</p>
<p>under certain conditions:</p>
<ul>
<li>f is convex(its Hessian is positive semi-definite)</li>
<li>$g_i$’s are convex set</li>
<li>$h_i$’s are affine, $h_i(w)&#x3D;a_i^Tw+b_i$</li>
</ul>
<h4 id="KKT-Conditions"><a href="#KKT-Conditions" class="headerlink" title="KKT Conditions"></a>KKT Conditions</h4><p>$$<br>\min_w f(w)<br>$$</p>
<p>$$<br>\text{s.t.}\ g_i(w) \le 0,i&#x3D;1,\dots,k<br>$$</p>
<p>$$<br>\ \ \ \ \ h_i(w)&#x3D;0,i&#x3D;1,\dots,l<br>$$</p>
<p>$$<br>\mathcal L(w,\alpha,\beta)&#x3D;f(w)+\sum_{i&#x3D;1}^k \alpha_i g_i(w)+\sum_{i&#x3D;1}^l \beta_i g_i(w)<br>$$</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial}{\partial w_i} \mathcal{L}\left(w^*, \alpha^*, \beta^<em>\right) &amp; &#x3D;0, \quad i&#x3D;1, \ldots, n \<br>\frac{\partial}{\partial \beta_i} \mathcal{L}\left(w^</em>, \alpha^*, \beta^<em>\right) &amp; &#x3D;0, \quad i&#x3D;1, \ldots, l \<br>\alpha_i^</em> g_i\left(w^<em>\right) &amp; &#x3D;0, \quad i&#x3D;1, \ldots, k \<br>g_i\left(w^</em>\right) &amp; \leq 0, \quad i&#x3D;1, \ldots, k \<br>\alpha^* &amp; \geq 0, \quad i&#x3D;1, \ldots,<br>\end{aligned}<br>$$</p>
<h4 id="SVM-from-Primal-to-Dual"><a href="#SVM-from-Primal-to-Dual" class="headerlink" title="SVM:from Primal to Dual"></a>SVM:from Primal to Dual</h4><p>$$<br>\begin{aligned}<br>\min _{w, b} &amp; \frac{1}{2}|w|^2 \<br>\text { s.t. } &amp; y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1, \quad i&#x3D;1, \ldots, m<br>\end{aligned}<br>$$</p>
<p><strong>Lagrange function</strong><br>$$<br>\mathcal L(w,b,a)&#x3D;\frac{1}{2}|w|^2-\sum_{i-1}^{m}\alpha_i[y^{(i)}\left(w^T x^{(i)}+b\right)-1]<br>$$</p>
<p>$$<br>\nabla_w \mathcal L(w,b,a)&#x3D;w-\sum_{i-1}^{m}\alpha_i y^{(i)}x^{(i)}&#x3D;0 \Rightarrow w^*&#x3D;\sum_{i&#x3D;1}^{m}\alpha_i y^{(i)}x^{(i)}<br>$$</p>
<p>$$<br>\frac{\partial}{\partial b}\mathcal L(w,b,a)&#x3D;\sum_{i&#x3D;1}^m \alpha_i^*y^{(i)}&#x3D;0<br>$$</p>
<p>将上述条件带入$\mathcal L$中，得到<br>$$<br>\mathcal{L}(w, b, \alpha)&#x3D;\sum_{i&#x3D;1}^m \alpha_i-\frac{1}{2} \sum_{i, j&#x3D;1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j\left(x^{(i)}\right)^T x^{(j)}<br>$$<br><strong>solving the Dual:The SMO Algorithm</strong><br>$$<br>\max_\alpha W(\alpha)&#x3D;\sum_{i&#x3D;1}^m \alpha_i-\frac{1}{2} \sum_{i, j&#x3D;1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j\left(x^{(i)}\right)^T x^{(j)}<br>$$</p>
<p>$$<br>\text{s.t.}\ \alpha_i \ge0,i&#x3D;1,\dots,m<br>$$</p>
<p>$$<br>\sum_{i&#x3D;1}^m \alpha_i y^{(i)}&#x3D;0<br>$$</p>
<p><img src="/imgs/ml/43.png" alt="image-20231025112704921"></p>
<h3 id="The-Dual-Hard-SVM"><a href="#The-Dual-Hard-SVM" class="headerlink" title="The Dual Hard SVM"></a>The Dual Hard SVM</h3><p>$$<br>\max_\alpha W(\alpha)&#x3D;\sum_{i&#x3D;1}^m \alpha_i-\frac{1}{2} \sum_{i, j&#x3D;1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j\left(x^{(i)}\right)^T x^{(j)}<br>$$</p>
<p>$$<br>\text{s.t.}\ \alpha_i \ge0,i&#x3D;1,\dots,m<br>$$</p>
<p>$$<br>\sum_{i&#x3D;1}^m \alpha_i y^{(i)}&#x3D;0<br>$$</p>
<p>得到$\alpha^*$后代入$w^*&#x3D;\sum_{i&#x3D;1}^{m}\alpha_i y^{(i)}x^{(i)}$得到最优解$w^*$</p>
<p><img src="/imgs/ml/44.png" alt="image-20231025113129561"></p>
<p><img src="/imgs/ml/45.png" alt="image-20231025113202692"></p>
<p><img src="/imgs/ml/46.png" alt="image-20231025113224064"></p>
<p><img src="/imgs/ml/47.png" alt="image-20231025113255025"></p>
<ul>
<li>一般情况下只有少数训练样本对应的Lagrange Multiplier大于零(支持向量)，分类面则是由这些支持向量决定</li>
<li>决策时只需计算新样本与所有支持向量的内积</li>
</ul>
<h3 id="From-Hard-SVM-to-Soft-SVM"><a href="#From-Hard-SVM-to-Soft-SVM" class="headerlink" title="From Hard SVM to Soft SVM"></a>From Hard SVM to Soft SVM</h3><p>$$<br>(w^*_{hard},b^*_{hard})&#x3D;\arg\min_{w,b}\frac{1}{2}|w|^2<br>$$</p>
<p>$$<br>\text { s.t. }  y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1, \quad i&#x3D;1, \ldots, m<br>$$</p>
<p>$$<br>(w^*_{hard},b^*_{hard})&#x3D;\arg\min_{w,b} \sum_{i&#x3D;1}^m \ell_{0-\infin}(y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1)+\frac{1}{2}|w|^2<br>$$</p>
<p>$$<br>J(\theta)&#x3D;L(\theta)+\lambda R(\theta)<br>$$</p>
<h3 id="From-Logistic-Loss-to-Hinge-Loss"><a href="#From-Logistic-Loss-to-Hinge-Loss" class="headerlink" title="From Logistic Loss to Hinge Loss"></a>From Logistic Loss to Hinge Loss</h3><p>$$<br>z&#x3D;\theta^Tx&#x3D;w^Tx+b<br>$$</p>
<p>$$<br>\text { Logistic Loss: } \ell&#x3D;\left{\begin{array}{ll}<br>-\log \left(\frac{1}{1+e^{-z}}\right)&#x3D;\log \left(1+e^{-z}\right), &amp; y&#x3D;+1 \<br>-\log \left(1-\frac{1}{1+e^{-z}}\right)&#x3D;\log \left(1+e^z\right), &amp; y&#x3D;-1<br>\end{array} \quad \Rightarrow \ell&#x3D;\log \left(1+e^{-y z}\right)\right.<br>$$</p>
<p>$$<br>\text{Hinge\ Loss}:\ell&#x3D;\max(1-yz,0)<br>$$</p>
<h3 id="The-Primal-Soft-SVM-problem"><a href="#The-Primal-Soft-SVM-problem" class="headerlink" title="The Primal Soft SVM problem"></a>The Primal Soft SVM problem</h3><p>Logistic loss<br>$$<br>(w^*_{hard},b^*_{hard})&#x3D;\arg\min_{w,b} \sum_{i&#x3D;1}^m \ell_{0-\infin}(y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1)+\frac{1}{2}|w|^2<br>$$<br>hinge loss<br>$$<br>(w^*_{soft},b^*_{soft})&#x3D;\arg\min_{w,b} C\sum_{i&#x3D;1}^m \max(1-y^{(i)}\left(w^T x^{(i)}+b\right),0 )+\frac{1}{2}|w|^2<br>$$<br><strong>松弛因子</strong><br>$$<br>\xi_i&#x3D;\max(1-y^{(i)}\left(w^T x^{(i)}+b\right),0 )<br>$$<br><strong>惩罚因子</strong>$C$<br>$$<br>(w^*_{soft},b^*_{soft})&#x3D;\arg\min_{w,b,\xi} C\sum_{i&#x3D;1}^m \frac{1}{2}|w|^2+C\sum_{i&#x3D;1}^m \xi_i<br>$$</p>
<p>$$<br>\text { s.t. }  y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1-\xi_i, \quad i&#x3D;1, \ldots, m<br>$$</p>
<p>$$<br>\xi_i \ge 0,i&#x3D;1,\dots,m<br>$$</p>
<h3 id="The-Dual-Soft-SVM"><a href="#The-Dual-Soft-SVM" class="headerlink" title="The Dual Soft SVM"></a>The Dual Soft SVM</h3><p>$$<br>(w^*_{soft},b^*_{soft})&#x3D;\arg\min_{w,b,\xi} C\sum_{i&#x3D;1}^m \frac{1}{2}|w|^2+C\sum_{i&#x3D;1}^m \xi_i<br>$$</p>
<p>$$<br>\mathcal L(w,b,\xi,\alpha,r)&#x3D;\frac{1}{2} w^Tw+C\sum_{i&#x3D;1}^m \xi_i-\sum_{i&#x3D;1}^m \alpha_i[y^{(i)}(x^Tw+b)-1+\xi_i]-\sum_{i&#x3D;1}^m r_i\xi_i<br>$$</p>
<p>求偏导&#x3D;0后<br>$$<br>w^*&#x3D;\sum_{i&#x3D;1}^m \alpha_iy^{(i)}x^{(i)}<br>$$</p>
<p>$$<br>\sum_{i&#x3D;1}^m \alpha_i^* y^{(i)}&#x3D;0<br>$$</p>
<p>$$<br>C&#x3D;\alpha_i+r_i<br>$$</p>
<p><strong>primal</strong><br>$$<br>(w^*_{soft},b^*_{soft})&#x3D;\arg\min_{w,b,\xi} C\sum_{i&#x3D;1}^m \frac{1}{2}|w|^2+C\sum_{i&#x3D;1}^m \xi_i<br>$$</p>
<p>$$<br>\text { s.t. }  y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1-\xi_i, \quad i&#x3D;1, \ldots, m<br>$$</p>
<p>$$<br>\xi_i \ge 0,i&#x3D;1,\dots,m<br>$$</p>
<p><strong>dual</strong><br>$$<br>\max_\alpha W(\alpha)&#x3D;\sum_{i&#x3D;1}^m \alpha_i-\frac{1}{2} \sum_{i, j&#x3D;1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j\left(x^{(i)}\right)^T x^{(j)}<br>$$</p>
<p>$$<br>\text{s.t.}\ 0 \le \alpha_i \le C,i&#x3D;1,\dots,m<br>$$</p>
<p>$$<br>\sum_{i&#x3D;1}^m \alpha_i y^{(i)}&#x3D;0<br>$$</p>
<p>对比hard，求解多一个惩罚因子C</p>
<h4 id="KTT-conditions"><a href="#KTT-conditions" class="headerlink" title="KTT conditions"></a>KTT conditions</h4><p><img src="/imgs/ml/48.png" alt="image-20231103165520948"></p>
<h4 id="Support-vectors-in-Soft-SVM"><a href="#Support-vectors-in-Soft-SVM" class="headerlink" title="Support vectors in Soft SVM"></a>Support vectors in Soft SVM</h4><p>$$<br>(w^*_{soft},b^*_{soft})&#x3D;\arg\min_{w,b,\xi} C\sum_{i&#x3D;1}^m \frac{1}{2}|w|^2+C\sum_{i&#x3D;1}^m \xi_i<br>$$</p>
<p>$$<br>\text { s.t. }  y^{(i)}\left(w^T x^{(i)}+b\right) \geq 1-\xi_i, \quad i&#x3D;1, \ldots, m<br>$$</p>
<p>$$<br>\xi_i \ge 0,i&#x3D;1,\dots,m<br>$$</p>
<p><img src="/imgs/ml/49.png" alt="image-20231103171213776"></p>
<p>The dual soft SVM 是一个二次规划问题，通过常用的QP算法求解，规模正比于训练样本数</p>
<p>使用SMO算法</p>
<p><img src="/imgs/ml/50.png" alt="image-20231103171443347"></p>
<p><img src="/imgs/ml/51.png" alt="image-20231103171554751"></p>
<h4 id="SMO-implementation"><a href="#SMO-implementation" class="headerlink" title="SMO implementation"></a>SMO implementation</h4><p><img src="/imgs/ml/52.png" alt="image-20231103172215902"></p>
<p><img src="/imgs/ml/53.png" alt="image-20231103172243757"></p>
<h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>通过引入松弛因子，Soft SVM能处理部分“特异点”outlier导致的线性不可分问题</p>
<p>如果数据本身是线性不可分的,显式将数据变换到新的空间(如采用极坐标、多项式升维)，使其线性可分，</p>
<p>$x \to \phi(x)$</p>
<p>对应SVM分类标准：$y&#x3D;sign(\sum_{i&#x3D;1}^m \alpha_i y^{(i)}&lt;\phi(x^{(i)}),\phi(x)&gt;+b)$</p>
<ul>
<li>只有少数的$\alpha &gt;0$</li>
<li>只需要知道测试样本x与支持所有支持向量的内积，无需明确知道对应的映射</li>
</ul>
<p>直接定义核函数计算内积<br>$$<br>k(x,z)&#x3D;&lt;\phi(x),\phi(z)&gt;&#x3D;\phi(x)^T\phi(z)<br>$$</p>
<h4 id="线性核"><a href="#线性核" class="headerlink" title="线性核"></a>线性核</h4><p>$$<br>k(x,z)&#x3D;(x^Tz)^2\ x,z \in \mathbb R^n<br>$$</p>
<p>$$<br>k(x,z)&#x3D;\sum_{i,j&#x3D;1}^n (x_ix_j)(z_iz_j)<br>$$</p>
<p>$$<br>k(x,z)&#x3D;(x^Tz+c)^2&#x3D;\sum_{i,j&#x3D;1}^n (x_ix_j)(z_iz_j)+\sum_{i&#x3D;1}^n (\sqrt{2c}x_i)(\sqrt{2c}z_i)+c^2<br>$$</p>
<h4 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a>多项式核</h4><p>$$<br>k(x,z)&#x3D;(x^T+c)^d<br>$$</p>
<h4 id="高斯核RBF"><a href="#高斯核RBF" class="headerlink" title="高斯核RBF"></a>高斯核RBF</h4><p>$$<br>k(x,z)&#x3D;\exp(-\frac{|x-z|^2}{2\sigma^2}), \phi(x) \in \mathbb R^\infin<br>$$</p>
<p>证明维度无穷</p>
<p><img src="/imgs/ml/54.png" alt="image-20231108082800372"></p>
<h4 id="Kernel-Matrix-and-Mercer-Kernel"><a href="#Kernel-Matrix-and-Mercer-Kernel" class="headerlink" title="Kernel Matrix and Mercer Kernel"></a>Kernel Matrix and Mercer Kernel</h4><p><img src="/imgs/ml/55.png" alt="image-20231108083244126"></p>
<p>假定存在某个核函数$K(\cdot,\cdot)$对应某个隐式映射$\phi(\cdot)$，给定训练集${ x^{(1)},\dots,x^{(m)} }$，对应的核矩阵$K \in \mathbb R^{m \times m}$为对称矩阵，其元素$K_{ij}&#x3D;K(x^{(i)},x^{(j)})$</p>
<p><img src="/imgs/ml/56.png" alt="image-20231108083910699"></p>
<h3 id="Multi-Class-SVM"><a href="#Multi-Class-SVM" class="headerlink" title="Multi-Class SVM"></a>Multi-Class SVM</h3><ul>
<li>将而分类器拓展处理多分类问题的基本思路<ul>
<li>训练：采用one vs. rest策略训练K个分类器$h_{\theta^{(k)}}(x)$</li>
<li>测试：选择分类器输出最大的值</li>
</ul>
</li>
</ul>
<p><img src="/imgs/ml/57.png" alt="image-20231108084442202"></p>
<p>采用joint optimization，保证各类的参数矢量$(w^{(k)},b^{(k)})$有同样的规模</p>
<h3 id="SVM-vs-Logistic-Regression"><a href="#SVM-vs-Logistic-Regression" class="headerlink" title="SVM vs. Logistic Regression"></a>SVM vs. Logistic Regression</h3><p><img src="/imgs/ml/58.png" alt="image-20231108085703531"></p>
<h3 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h3><ul>
<li><p>soft SVM classifier<br>$$<br>\min_{w,b}C\sum_{i&#x3D;1}^m \max (1-y^{(i)}(w^Tx^{(i)}+b),0)+\frac{1}{2}| w |^2<br>$$</p>
</li>
<li><p>一般形式<br>$$<br>\min_{w,b}C\sum_{i&#x3D;1}^m \ell (y^{(i)},h_{w,b}(x^{i}))+\frac{1}{2}| w |^2<br>$$</p>
</li>
</ul>
<p><img src="/imgs/ml/59.png" alt="image-20231108090123532"></p>
<p>不敏感损失函数</p>
<p><img src="/imgs/ml/60.png" alt="image-20231108090229107"></p>
<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p>引入松弛因子$\xi_i,\xi_i^*$</p>
<p><img src="/imgs/ml/61.png" alt="image-20231108090705313"><br>$$<br>h_{w,b}(x)&#x3D;w^Tx+b&#x3D;&lt;w,x&gt;+b<br>$$<br><img src="/imgs/ml/62.png" alt="image-20231108090734302"></p>
<p><img src="/imgs/ml/63.png" alt="image-20231108091034083"></p>
<p><img src="/imgs/ml/64.png" alt="image-20231108091101862"></p>
<p><img src="/imgs/ml/65.png" alt="image-20231108091130570"></p>
<p><img src="/imgs/ml/66.png" alt="image-20231108091149718"></p>
<h4 id="非线性回归"><a href="#非线性回归" class="headerlink" title="非线性回归"></a>非线性回归</h4><p>引入核函数<br>$$<br>h_{w,b}(x)&#x3D;\sum_{i&#x3D;1}^m (\alpha_i^*-\alpha)&lt;\phi(x^{(i)}),\phi(x)&gt;+b&#x3D;\sum_{i&#x3D;1}^m (\alpha_i^*-\alpha)k(x^{(i)},x)+b<br>$$</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><ul>
<li>分类问题一般包括两个步骤<ul>
<li>模型构建（归纳）：通过对训练集合的归纳，建立分类模型</li>
<li>预测应用（推论）：根据建立的分类模型，对测试集合进行测试</li>
</ul>
</li>
<li>决策树是一种典型的分类方法<ul>
<li>首先对数据进行处理，利用归纳算法生成可读的规则和决策树</li>
<li>然后使用决策对新数据进行分析</li>
<li>本质上是通过一系列规则对数据进行分类的过程</li>
<li>相关算法：CLS，ID3，C4.5，CART</li>
</ul>
</li>
</ul>
<h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>决策树（decision tree）具有树结构</p>
<ul>
<li>树的最顶层是根节点，包含所有样本</li>
<li>每个内部节点表示在一个特征（或属性）上的测试，将当前节点的样本划分到不同的子节点中；</li>
<li>每个叶节点代表类或类分布，对应决策结果</li>
<li>使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果</li>
</ul>
<h3 id="CLS算法"><a href="#CLS算法" class="headerlink" title="CLS算法"></a>CLS算法</h3><h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><p>从一棵空决策树开始，选择某一属性（分类属性）作为测试属性。该测试属性对应决策树中的决策结点。根据该属性的值的不同，可将训练样本分成相应的子集</p>
<ul>
<li>如果该子集为空，或该子集中的样本属于同一个类，则该子集为叶结点</li>
<li>否则该子集对应于决策树的内部结点，即测试结点，需要选择一个新的分类属性对该子集进行划分，直到所有的子集都为空或者属于同一类</li>
</ul>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li>生成一颗空决策树和一张训练样本属性集;</li>
<li>若训练样本集D中所有的样本都属于同一类,则生成结点T , 并终止学习算法;否则</li>
<li>根据某种策略从训练样本属性表中选择属性a作为测试属性, 生成测试结点若a的取值为v1,v2,…,vm, 则根据a 的取值的不同,将D划分成m个子集D1,D2,…,Dm;</li>
<li>从训练样本属性表中删除属性a;</li>
<li>转步骤2, 对每个子集递归调用CLS;</li>
</ol>
<p>关键：在步骤3中，如何根据某种策略从训练样本属性表中选择属性a作为测试属性</p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><ul>
<li><p>Shannon提出信息论理论</p>
</li>
<li><p>熵entropy：信息量大小的度量，即标识随机变量不确定性的度量</p>
</li>
<li><p>事件$a_i$的信息量$I(a_i)$如下度量：$I(a_{i})&#x3D;-\log p(a_{i})$,这里$p(a_i)$为事件$a_i$发生的概率</p>
</li>
<li><p>假设有n个互不相容的事件$a_1,a_2,\dots,a_n$中有且仅有一个发生，则其平均的信息量如下度量<br>$$<br>I(a_1,\cdots,a_n)&#x3D;\sum_iI(a_i)&#x3D;-\sum_ip(a_i)\log p(a_i)<br>$$</p>
</li>
<li><p>假设当前样本集D中第k类样本的比例为$p_k$，对应的信息熵为<br>$$<br>Ent(D)&#x3D;-\sum_kp_k\log p_k<br>$$</p>
</li>
<li><p>$Ent(D)$越小，表示数据越有序，纯度越高，分类效果越好</p>
</li>
<li><p>假设某离散属性$a$有V个可能值，若采用该属性对样本集来划分，则会产生V个分支，每个分支节点包含的数据记为$D^v$</p>
</li>
<li><p>用属性$a$对样本集$D$进行划分，获得的信息增益为：</p>
<p>$$<br>Gain(D,a)&#x3D;Ent(D)-\sum_v\frac{|D^v|}{|D|}Ent(D^v)<br>$$</p>
</li>
<li><p>选择具有最大信息增益的属性来划分： $a^*&#x3D;\arg\max_aGain(D,a)$ (ID3)</p>
</li>
</ul>
<h3 id="ID3决策树生成算法"><a href="#ID3决策树生成算法" class="headerlink" title="ID3决策树生成算法"></a>ID3决策树生成算法</h3><ol>
<li><p>决定分类属性；</p>
</li>
<li><p>对目前的数据表，建立一个节点N</p>
</li>
<li><p>如果数据库中的数据都属于同一个类，N就是树叶，在树叶上标出所属的类</p>
</li>
<li><p>如果数据表中没有其他属性可以考虑，则N也是树叶，按照少数服从多数的原则在树叶上标出所属类别</p>
</li>
<li><p>否则，根据平均信息期望值GAIN值选出一个最佳属性作为节点N的测试属性</p>
</li>
<li><p>节点属性选定后，对于该属性中的每个值</p>
</li>
</ol>
<ul>
<li>从N生成一个分支，并将数据表中与该分支有关的数据收集形成分支节点的数据表，在表中删除节点属性那一栏如果分支数据表非空，则运用以上算法从该节点建立子树</li>
</ul>
<p>ID3算法的基本思想是以信息增益选择属性，实际应用中会对可能取值数目较多的属性有所偏好</p>
<ul>
<li>对每个训练样本进行编号，并将该编号作为属性，其信息增益最大，但显然该属性不能作为分类依据</li>
</ul>
<h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>$$<br>\operatorname{Gain_ratio}(D,a)&#x3D;\frac{\operatorname{Gain}(D,a)}{IV(a)},<br>$$</p>
<p>其中<br>$$<br>IV(a)&#x3D;-\sum_{v&#x3D;1}^{V}\frac{|D^v|}{|D|}\log\frac{|D^v|}{|D|}<br>$$</p>
<p>为属性$a$的intrinsic value, $a$可能的取值越多，$IV(a)$通常也越大。</p>
<h3 id="决策树C4-5的生成算法"><a href="#决策树C4-5的生成算法" class="headerlink" title="决策树C4.5的生成算法"></a>决策树C4.5的生成算法</h3><p><img src="/imgs/ml/67.png" alt="image-20231110170323430"></p>
<h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>决策树剪枝减少决策树的规模是处理过拟合问题的主要手段</p>
<p>通过极小化决策树整体的Cost Complexity函数实现：<br>$$<br>CC(T)&#x3D;Err(T)+\lambda R(T)<br>$$<br>这里$Err(T)$为树$T$的错误，$R(T)$为正则项，描述树的复杂度(如树节点的个数), $\lambda\geq0$为正则化参数</p>
<p>如： 设树$T$的叶结点个数为$|T|$,$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k&#x3D;1,2,\cdots,K$。叶结点$t$上的经验熵：<br>$$<br>H_{t}(T)&#x3D;-\sum_{k&#x3D;1}^{K}\frac{N_{tk}}{N_{t}}\log\frac{N_{tk}}{N_{t}}<br>$$<br>目标函数为：<br>$$<br>CC(T)&#x3D;\sum_{i&#x3D;1}^{|T|}N_{t}H_{t}(T)+\lambda|T|&#x3D;-\sum_{i&#x3D;1}^{|T|}\sum_{k&#x3D;1}^{K}N_{tk}\log\frac{N_{tk}}{N_{t}}+\lambda|T|<br>$$<br><strong>算法</strong></p>
<ul>
<li>输入：生成算法产生的整个决策树T，正则参数$\lambda$</li>
<li>输出：<ol>
<li>计算每个节点的代价</li>
<li>递归地从树的叶节点向上回溯</li>
</ol>
</li>
<li>设一组叶节点回缩到其父节点之前与之后的代价分别为： $CC(T_B)$和$CC(T_A)$,若$CC(T_A)\leq CC(T_B)$,则剪枝，返回Step 2,直到不能继续为止。</li>
<li>根据验证集上的代价决定是否剪枝</li>
</ul>
<h3 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h3><ul>
<li>分类回归树(Classification and Regression Tree, CART)<ul>
<li>分类树：Gini Index</li>
<li>回归树：平方误差最小化</li>
</ul>
</li>
<li>CART算法由两部分组成<ul>
<li>决策树生成</li>
<li>决策树剪枝</li>
</ul>
</li>
<li>CART vs. ID3<ul>
<li>二元划分：二叉树不易产生数据碎片，精确度往往也会高于多叉树</li>
<li>属性选择：<ul>
<li>Gini Index - 分类树</li>
<li>最小平方残差- 回归树</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="基尼系数与CART决策树"><a href="#基尼系数与CART决策树" class="headerlink" title="基尼系数与CART决策树"></a>基尼系数与CART决策树</h4><p>CART决策树采用基尼系数来选择属性进行划分</p>
<p>基尼值：<br>$$<br>\mathrm{Gini}(D)&#x3D;\sum_{k&#x3D;1}^K\sum_{k^{\prime}\neq k}p_kp_{k^{\prime}}&#x3D;1-\sum_{k&#x3D;1}^Kp_k^2<br>$$<br>直观上，基尼值反映了从数据集中任选两个样本，其类别不一致的概率，其值越小，纯度越高</p>
<p>基尼系数：<br>$$<br>\operatorname{Gini_index}(D,a)&#x3D;\sum_{v&#x3D;1}^{V}\frac{|D^v|}{|D|}\mathrm{Gini}(D^v)<br>$$</p>
<h4 id="多属性值的处理"><a href="#多属性值的处理" class="headerlink" title="多属性值的处理"></a>多属性值的处理</h4><ul>
<li>组合的方式转化成多个二取值问题：如（青年、中老年）、（中年、青老年）、（中青年、老年）等</li>
<li>然后分别计算Gini Index，选择Gini Index最小的二分情况</li>
</ul>
<h4 id="连续属性的处理"><a href="#连续属性的处理" class="headerlink" title="连续属性的处理"></a>连续属性的处理</h4><ul>
<li><p>C4.5决策树和CART决策树均采用将连续属性离散化，最简单的是二分法</p>
</li>
<li><p>虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的</p>
</li>
<li><p>给定样本集$D$和某连续属性$a$,假定$a$在$D$上出现了$n$个不同的取值，由大到小排序后记为${a^1,a^2,\cdots,a^n}.$</p>
</li>
<li><p>基于阈值$t$可把$D$划分为$D_t^-$和$D_t^+$两个子集，其中$D_t^-$包含哪些属性$a$取值小于$t$的那部分样本，$D_t^+$包含哪些属性$a$取值大于$t$的那部分样本</p>
</li>
<li><p>显然，对于任意的$t\in(a^i,a^{i+1})$,效果相同，因此可遍历如下阈值集合${\frac{1}{2}(a^{i}+a^{i+1})|1\leq i\leq n-1}$,根据Gini Index或Gain Ratio来选择最优的阈值$t$进行离散化</p>
</li>
</ul>
<h4 id="生成算法"><a href="#生成算法" class="headerlink" title="生成算法"></a>生成算法</h4><p><img src="/imgs/ml/68.png" alt="image-20231110172317645"></p>
<h4 id="CART回归树的生成"><a href="#CART回归树的生成" class="headerlink" title="CART回归树的生成"></a>CART回归树的生成</h4><p><img src="/imgs/ml/69.png" alt="image-20231110172424721"></p>
<p><img src="/imgs/ml/70.png" alt="image-20231110172517395"></p>
<p><img src="/imgs/ml/71.png" alt="image-20231110172550716"></p>
<h4 id="CART的剪枝算法"><a href="#CART的剪枝算法" class="headerlink" title="CART的剪枝算法"></a>CART的剪枝算法</h4><p><img src="/imgs/ml/72.png" alt="image-20231110172635638"></p>
<p><img src="/imgs/ml/73.png" alt="image-20231110172705725"></p>
<p><img src="/imgs/ml/74.png" alt="image-20231110172728641"></p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>构建多个学习器一起结合来完成具体的学习任务</p>
<p><img src="/imgs/ml/75.png" alt="image-20231122090210698"></p>
<p><img src="/imgs/ml/76.png" alt="image-20231122090243070"></p>
<p>通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能，对“弱学习器” 尤为明显</p>
<p>集成学习分类</p>
<ul>
<li>个体学习器间存在强依赖关系，必须串行生成的序列化方法。代表：Boosting (AdaBoost,Gradient Boosting Machine)</li>
<li>个体学习器间不存在强依赖关系，可同时生成的并行化方法。代表：Bagging和随机森林（Random Forest）</li>
</ul>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h4><p><img src="/imgs/ml/77.png" alt="image-20231122092206246"></p>
<ul>
<li>基分类器：$h_t$</li>
<li>强分类器：$H$</li>
<li>每次在学习$h_t$的时候，更关注分类器$h_{t-1}$错分的样本</li>
<li>从偏差-方差分解的角度看，AdaBoost主要关注降低错误率（即降低偏差），因此AdaBoost能基于分类性能相当弱的学习器构建出分类性能很强的分类器。</li>
</ul>
<p>加性模型：<br>$$<br>H(x)&#x3D;\sum_{t&#x3D;1}^{T}\alpha_{t}h_{t}(x)<br>$$<br><img src="/imgs/ml/78.png" alt="image-20231122093600297"></p>
<p><strong>AdaBoost算法的推导</strong></p>
<p>假设函数：$H(x)&#x3D;\sum_{t&#x3D;1}^T\alpha_th_t(x)$</p>
<p>目标函数：<br>$$<br>\begin{aligned}<br>(\alpha_{t},h_{t})&amp; &#x3D;\arg\min_{\alpha,h}\frac{1}{m}\sum_{i&#x3D;1}^{m}\exp\left[-y^{(i)}\left(H_{t-1}(x^{(i)})+\alpha h(x^{(i)})\right)\right]  \<br>&amp;&#x3D;\arg\min\limits_{\alpha,h}\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\exp\left[-y^{(i)}\left(\sum\limits_{\tau&#x3D;1}^{t-1}\alpha_{\tau}h_{\tau}(x^{(i)})+\alpha h(x^{(i)})\right)\right]<br>\end{aligned}<br>$$<br>损失函数：$\ell(H(x),y)&#x3D;\exp(-yH(x))$</p>
<p>第一个分类器$h_1$直接基于初始数据分布用基学习算法可得。此后迭代生产$h_t$和对应的权重$\alpha_th_t$最小化指数损失<br>$$<br>\begin{aligned}\ell_t(\alpha_t)&amp;&#x3D;E_{x\sim\mathcal{D}<em>t}\exp[-y\alpha_th_t(x)]\&amp;&#x3D;e^{-\alpha_t}P</em>{x\sim\mathcal{D}<em>t}[h_t(x)&#x3D;y]+e^{\alpha_t}P</em>{x\sim\mathcal{D}<em>t}[h_t(x)\neq y]&#x3D;(1-\epsilon_t)e^{-\alpha_t}+\epsilon_te^{\alpha_t}\end{aligned}<br>$$<br>令$\frac{\partial\ell</em>{t}}{\partial\alpha_{t}}&#x3D;-e^{-\alpha_{t}}(1-\epsilon_{t})+e^{\alpha_{t}}\epsilon_{t}&#x3D;0$，有$\alpha_t&#x3D;\frac{1}{2}\log\left(\frac{1-\epsilon_t}{\epsilon_t}\right)$</p>
<p>得到分类器$H_{t-1}$后，分类器$h_t$应能纠正$H_{t-1}$的错误，即应最小化</p>
<p>$y \in { -1,+1 },h_t(x) \in { -1,+1 }$<br>$$<br>\ell_{t}(H_{t-1}+h_{t}|\mathcal{D})&#x3D;E_{x\sim\mathcal{D}}\exp[-y(H_{t-1}(x)+h_{t}(x))]&#x3D;E_{x\sim\mathcal{D}}e^{-yH_{t-1}(x)}e^{-yh_{t}(x)}<br>$$<br>根据泰勒公式有<br>$$<br>e^{-yh_t(x)}\approx1-yh_t(x)+\frac{1}{2}y^2h_t^2(x)&#x3D;\frac{3}{2}-yh_t(x)<br>$$</p>
<p>$$<br>\begin{aligned}<br>h_{t}(x)&amp; &#x3D;\arg\operatorname*{min}<em>{h}E</em>{x\sim\mathcal{D}}e^{-yH_{t-1}(x)}e^{-yh_{t}(x)}  \<br>&amp;\approx\arg\operatorname*{max}<em>{h}E</em>{x\sim\mathcal{D}}e^{-yH_{t-1}(x)}yh_{t}(x)&#x3D;\arg\operatorname*{max}<em>{h}E</em>{x\sim\mathcal{D}}\left[\frac{e^{-yH_{t-1}(x)}}{E_{x\sim\mathcal{D}}e^{-yH_{t-1}(x)}}yh_{t}(x)\right]<br>\end{aligned}<br>$$</p>
<p>令$\mathcal D_t$表示分布<br>$$<br>\mathcal{D}<em>{t}(x)&#x3D;\frac{\mathcal{D}(x)e^{-yH</em>{t-1}(x)}}{E_{x\sim\mathcal{D}}e^{-yH_{t-1}(x)}}<br>$$<br>根据期望定义<br>$$<br>\begin{aligned}<br>h_{t}(x)&amp; &#x3D;\arg\max_{h}E_{x\sim\mathcal{D}<em>{t}}[yh</em>{t}(x)]  \<br>&amp;&#x3D;\arg\min_{h}E_{x\sim\mathcal{D}<em>{t}}\mathbb{I}[y\neq h</em>{t}(x)]<br>\end{aligned}<br>$$<br>$\mathcal D_t$和$\mathcal D_{t+1}$有<br>$$<br>\begin{aligned}<br>\mathcal{D}<em>{t+1}(x)&amp; &#x3D;\frac{\mathcal{D}(x)e^{-yH</em>{t}(x)}}{E_{x\sim\mathcal{D}}e^{-yH_{t}(x)}}  \<br>&amp;&#x3D;\frac{\mathcal{D}(x)e^{-yH_{t-1}(x)}e^{-y\alpha_{t}h_{t}(x)}}{E_{x\sim\mathcal{D}}e^{-yH_{t}(x)}} \<br>&amp;&#x3D;\mathcal{D}<em>{t}(x)e^{-y\alpha</em>{t}h_{t}(x)}\frac{E_{x\sim\mathcal{D}}e^{-yH_{t-1}(x)}}{E_{x\sim\mathcal{D}}e^{-yH_{t}(x)}}<br>\end{aligned}<br>$$<br>选择基学习器$h_t$</p>
<ul>
<li>decision stump $h_{s,i,\theta}&#x3D;s\operatorname{sign}(x_i-\theta)$</li>
<li>三个参数：feature i，threshold $\theta$，direction s</li>
<li>2D平面上水平或垂直线</li>
</ul>
<h4 id="Gradient-Boosting-Machine"><a href="#Gradient-Boosting-Machine" class="headerlink" title="Gradient Boosting Machine"></a>Gradient Boosting Machine</h4><p>采用加性模型，但是可以采用其他任意损失$\ell$</p>
<p>目标函数：<br>$$<br>(\alpha_{t},h_{t})&#x3D;\arg\min_{h,\alpha}\frac{1}{m}\sum_{i&#x3D;1}^{m}\ell\left(\sum_{\tau&#x3D;1}^{t-1}\alpha_{\tau}h_{\tau}(x^{(i)})+\alpha h(x^{(i)}),y^{(i)}\right)<br>$$</p>
<h5 id="Gradient-Boosting-Decision-Tree"><a href="#Gradient-Boosting-Decision-Tree" class="headerlink" title="Gradient Boosting Decision Tree"></a>Gradient Boosting Decision Tree</h5><p>采用决策树（回归树）作为基学习器</p>
<p>针对不同问题使用不同的损失函数：</p>
<ul>
<li>用指数损失函数的分类问题</li>
<li>用平方误差损失函数的回归问题</li>
</ul>
<p>$$<br>H(x)&#x3D;\sum_{t&#x3D;1}^{T}\alpha_{t}h_{t}(x)<br>$$</p>
<p>GBM的思想是$h_t$应能沿着损失函数负梯度降低损失函数的值</p>
<p>分类器$h_t$常采用CART树，每次学习一颗CART树，去拟合样本余量<br>$$<br>\tilde{y}^{(i)}&#x3D;-\left[\frac{\partial\ell(H,y^{(i)})}{\partial H}\right]<em>{H&#x3D;H</em>{t-1}(x^{(i)})}<br>$$<br><img src="/imgs/ml/79.png" alt="image-20231122111700294"></p>
<p><img src="/imgs/ml/80.png" alt="image-20231122111730500"></p>
<p>为了消除过拟合，加入正则项<br>$$<br>\begin{gathered}<br>(\alpha_{t},h_{t}) &#x3D;\arg\min_{\alpha,h}\frac{1}{m}\sum_{i&#x3D;1}^{m}\ell\left(\sum_{\tau&#x3D;1}^{t-1}\alpha_{\tau}h_{\tau}(x^{(i)})+\alpha h(x^{(i)}),y^{(i)}\right)+\lambda R\left(h\right) \<br>&#x3D;\arg\min\limits_{\alpha,h}\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}\ell\left(H_{t-1}(x^{(i)})+\alpha h(x^{(i)}),y^{(i)}\right)+\lambda R\left(h\right)<br>\end{gathered}<br>$$</p>
<h5 id="AdaBoost-vs-GBM"><a href="#AdaBoost-vs-GBM" class="headerlink" title="AdaBoost vs GBM"></a>AdaBoost vs GBM</h5><p>adaboost</p>
<p><img src="/imgs/ml/81.png" alt="image-20231122111908950"></p>
<p>GBM</p>
<p><img src="/imgs/ml/82.png" alt="image-20231122111934222"></p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p><img src="/imgs/ml/83.png" alt="image-20231122112028139"></p>
<p>自助采样(Bootstrap Sampling):指任何一种有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中</p>
<p>Bagging: 利用自助采样得到T组训练样本集，分别利用这些训练样本集训练T个分类器(CART or SVM or others)，最后进行投票集成</p>
<p>从Bias-Variance分解的角度看，Bagging主要关注降低方差</p>
<p><img src="/imgs/ml/84.png" alt="image-20231122112124395"></p>
<h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p><img src="/imgs/ml/85.png" alt="image-20231122112200167"></p>
<h3 id="决策融合策略"><a href="#决策融合策略" class="headerlink" title="决策融合策略"></a>决策融合策略</h3><p><img src="/imgs/ml/86.png" alt="image-20231122112224820"></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p><strong>非监督学习</strong></p>
<ul>
<li>同一簇中的样本尽可能的相似，而不同簇中的样本尽可能不同</li>
<li>高类内相似性(high intraclass similarity)</li>
<li>低类间相似性(low interclass similarity)</li>
<li>属于最典型的非监督学习方法</li>
</ul>
<h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><h4 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h4><p>$x \in \mathbb{R}^n,y \in \mathbb R^n,z \in \mathbb R^n,D(x,z)$为一个距离度量需要满足如下条件</p>
<ul>
<li>对称性：$D(x,z)&#x3D;D(z,x)$</li>
<li>正定性：$D(x,z)&#x3D;0 \ if \ only \ x&#x3D;z$</li>
<li>自距离为0：$D(x,x)&#x3D;0$</li>
<li>非负性：$D(x,z) \ge 0$</li>
<li>三角不等式：$D(x,z) \le D(x,y)+ D(y,z)$</li>
</ul>
<h5 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h5><p>$$<br>{\mathcal D}<em>{mk}(x,z)&#x3D;(\sum</em>{i&#x3D;1}^{n}|x_{i}-z_{i}|^{p})^{\frac{1}{p}}<br>$$</p>
<p>p&#x3D;2为欧氏距离<br>$$<br>D_{ed}(x,z)&#x3D;|x-z|&#x3D;\sqrt{\sum_{i&#x3D;1}^n|x_i-z_i|^2}<br>$$<br>p&#x3D;1为曼哈顿距离（城市距离）<br>$$<br>D_{man}(x,z)&#x3D;|x-z|<em>1&#x3D;\sum</em>{i&#x3D;1}^n|x_i-z_i|<br>$$<br>$p&#x3D;+\infin$为sup距离<br>$$<br>D_{sup}(x,z)&#x3D;\left|x-z\right|<em>\infty&#x3D;\max</em>{i&#x3D;1}^n\left|x_i-z_i\right|<br>$$</p>
<h5 id="Hamming距离"><a href="#Hamming距离" class="headerlink" title="Hamming距离"></a>Hamming距离</h5><p>当特征为二值特征时， Minkowski距离称为Hamming距离</p>
<h4 id="相似性度量"><a href="#相似性度量" class="headerlink" title="相似性度量"></a>相似性度量</h4><h5 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h5><p>$$<br>S_p(x,z)&#x3D;\frac{\sum_{i&#x3D;1}^n(x_i-\bar{x})(z_i-\bar{z})}{\sqrt{\sum_{i&#x3D;1}^n(x_i-\bar{x})^2\times\sum_{i&#x3D;1}^n(z_i-\bar{z})^2}}<br>$$</p>
<h5 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h5><p>$$<br>S_c(x,z)&#x3D;\frac{x^Tz}{|x||z|}<br>$$</p>
<h4 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h4><p><img src="/imgs/ml/90.png" alt="image-20231201175512564"></p>
<h5 id="一维高斯分布"><a href="#一维高斯分布" class="headerlink" title="一维高斯分布"></a>一维高斯分布</h5><p>$$<br>x\in\mathbb{R},x\sim\mathcal{N}(\mu,\sigma^{2})<br>$$</p>
<p>$$<br>\begin{aligned}p(x;\mu,\sigma)&amp;&#x3D;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\E(x)&#x3D;\mu\\text{var}(x)&#x3D;\sigma^2\end{aligned}<br>$$</p>
<h5 id="多维高斯分布"><a href="#多维高斯分布" class="headerlink" title="多维高斯分布"></a>多维高斯分布</h5><p>$$<br>x\in\mathbb{R}^{n},x\sim\mathcal{N}(\mu,\Sigma)<br>$$</p>
<p>$$<br>p(x;\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}\sqrt{|\Sigma|}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)<br>$$</p>
<p><img src="/imgs/ml/91.png" alt="image-20231201180211230"></p>
<h4 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h4><h5 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h5><p>$$<br>p(x)&#x3D;\sum_{j&#x3D;1}^Kp(x|z&#x3D;j)p(z&#x3D;j)<br>$$</p>
<p><img src="/imgs/ml/92.png" alt="image-20231206090919898"></p>
<h5 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h5><p>$$<br>h(x)&#x3D;\arg\max_kp(z&#x3D;k|x)&#x3D;\arg\max_k\frac{p(x|z&#x3D;k)p(z&#x3D;k)}{\sum_jp(x|z&#x3D;j)p(z&#x3D;j)}<br>$$</p>
<h5 id="高斯贝叶斯分类器"><a href="#高斯贝叶斯分类器" class="headerlink" title="高斯贝叶斯分类器"></a>高斯贝叶斯分类器</h5><p>$$<br>h(x)&#x3D;\arg\max_{k}\frac{p(x;\mu_{k},\Sigma_{k})p(z&#x3D;k)}{\sum_{j}p(x;\mu_{j},\Sigma_{j})p(z&#x3D;j)}<br>$$</p>
<p>$$<br>p(x|z&#x3D;j)&#x3D;p(x;\mu_j,\Sigma_j)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}\sqrt{|\Sigma_j|}}\exp\left(-\frac{1}{2}(x-\mu_j)^T\Sigma_j^{-1}(x-\mu_j)\right)<br>$$</p>
<h5 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h5><p>各特征相互独立<br>$$<br>p(z&#x3D;k|x)&#x3D;\frac{p(x|z&#x3D;k)p(z&#x3D;k)}{p(x)}&#x3D;\frac{p(z&#x3D;k)}{p(x)}\prod_{i&#x3D;1}^{n}p(x_{i}|z&#x3D;k)<br>$$</p>
<p>$$<br>h(x)&#x3D;\arg\max_kp(z&#x3D;k)\prod_{i&#x3D;1}^np(x_i|z&#x3D;k)<br>$$</p>
<h4 id="矩阵微分"><a href="#矩阵微分" class="headerlink" title="矩阵微分"></a>矩阵微分</h4><h5 id="行列式的微分"><a href="#行列式的微分" class="headerlink" title="行列式的微分"></a>行列式的微分</h5><p>$$<br>\frac{\partial|\Sigma|}{\partial\Sigma}&#x3D;|\Sigma|(\Sigma^{-1})^{T}<br>$$</p>
<h5 id="矩阵逆的微分"><a href="#矩阵逆的微分" class="headerlink" title="矩阵逆的微分"></a>矩阵逆的微分</h5><p>$$<br>\frac{\partial x^T\Sigma^{-1}y}{\partial\Sigma}&#x3D;-\Sigma^{-T}xy^T\Sigma^{-T}<br>$$</p>
<h4 id="凸函数-Convex-Function"><a href="#凸函数-Convex-Function" class="headerlink" title="凸函数 Convex Function"></a>凸函数 Convex Function</h4><p>$$<br>f(tx_1+(1-t)x_2)\leq tf(x_1)+(1-t)f(x_2)<br>$$</p>
<p>&#x3D;&gt;$f’’(x) \ge 0, \forall x$</p>
<p>&#x3D;&gt;矢量则Hessian矩阵H半正定$H \ge 0$</p>
<p><strong>严格凸</strong><br>$$<br>f(tx_1+(1-t)x_2)\lt tf(x_1)+(1-t)f(x_2)<br>$$<br>矢量则Hessian矩阵H半正定$H \gt 0$</p>
<p>严格凸函数中<br>$$<br>E(f(X)) &#x3D; f(E(X))<br>$$</p>
<p>当且仅当<br>$$<br>X&#x3D;E(X)<br>$$<br>对于上述逆反命题成立</p>
<h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>K 聚类簇的个数</p>
<p>训练集${ x^{(1)},x^{(2)},\dots,x^{(m)} }$</p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><ol>
<li><p>随机指定K个类的中心(seed)$\mu_1,\mu_2,\dots,\mu_K \in \mathbb R^n$</p>
</li>
<li><p>for i&#x3D;1 to m:</p>
<p>最近邻分类器进行分类：计算到$x^{(i)}$距离最近的聚类簇的中心，将其作为$x^{(i)}$的类别$y^{(i)} \in { 1,2,\dots,K }$，即$y^{(i)}&#x3D;\arg\min_k |x^{(i)}-\mu_k|^2$</p>
</li>
<li><p>for k&#x3D;1 to K</p>
<p>更新聚类簇的中心：用所有属于第k个簇的样本的均值去更新$\mu_k$，即$\mu_k&#x3D;avg(x^{(i)}|y^{(i)}&#x3D;k)$</p>
</li>
</ol>
<h4 id="算法的目标函数"><a href="#算法的目标函数" class="headerlink" title="算法的目标函数"></a>算法的目标函数</h4><p>$$<br>J(y^{(1)},\cdots,y^{(m)},\mu_{1},\cdots,\mu_{K})&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}|x^{(i)}-\mu_{y^{(i)}}|^{2}<br>$$</p>
<p><img src="/imgs/ml/87.png" alt="image-20231201173655096"><br>$$<br>\min_{\begin{array}{c}y^{(1)},\cdots,y^{(m)}\\mu_1,\cdots,\mu_K\end{array}}\frac{1}{m}\sum_{i&#x3D;1}^{m}|x^{(i)}-\mu_{y^{(i)}}|^2<br>$$<br>采用坐标下降法</p>
<p>已知$\mu_k$易求$y^{(i)}$，反之亦然</p>
<p>EM算法的特例，保证收敛到局部极小</p>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>计算两点距离$O(n)$</li>
<li>最近邻m个样本到K个中心的距离计算$O(Kmn)$</li>
<li>更新聚类簇中心$O(mn)$</li>
<li>需要$l$次迭代，总时间复杂度是$O(lKmn)$</li>
</ul>
<h4 id="初始化聚类簇中心"><a href="#初始化聚类簇中心" class="headerlink" title="初始化聚类簇中心"></a>初始化聚类簇中心</h4><ul>
<li>最终的聚类结果依赖于初始化，不同的初始化可能得到不同的聚类结果</li>
<li>随机初始化</li>
</ul>
<h4 id="如何选择聚类个数K"><a href="#如何选择聚类个数K" class="headerlink" title="如何选择聚类个数K"></a>如何选择聚类个数K</h4><p><img src="/imgs/ml/88.png" alt="image-20231201174847240"></p>
<p>很多情况下K-means只是一个复杂任务的前处理步骤（如经典的Bag of Words分类方法），因此可能需要结合最终任务的性能来调整对应的K</p>
<p><img src="/imgs/ml/89.png" alt="image-20231201174956655"></p>
<p>K-means算法依赖于距离度量的选择</p>
<ul>
<li>常采用欧氏距离</li>
<li>Distance Metric Learning</li>
<li>Kernel K-means：通过隐式映射$\phi$将数据变换到某个空间后，再用欧氏距离度量</li>
</ul>
<h4 id="Kernel-K-means"><a href="#Kernel-K-means" class="headerlink" title="Kernel K-means"></a>Kernel K-means</h4><p>$$<br>\mu_{k}&#x3D;\frac{1}{\gamma_{k}}\sum_{y^{(i)}&#x3D;k}\phi(x^{(i)}),<br>$$</p>
<p>$$<br>\begin{aligned}<br>|\phi(x)-\mu_{k}|^{2}&amp; &#x3D;|\phi(x)-\frac{1}{\gamma_{k}}\sum_{y^{(i)}&#x3D;k}\phi(x^{(i)})|^{2}  \<br>&amp;&#x3D;\left(\phi(x)-\frac1{\gamma_{k}}\sum_{y^{(i)}-k}\phi(x^{(i)})\right)^{T}\left(\phi(x)-\frac1{\gamma_{k}}\sum_{y^{(i)}-k}\phi(x^{(i)})\right) \<br>&amp;&#x3D;\phi(x)^{T}\phi(x)-2\frac{1}{\gamma_{k}}\sum_{y^{(i)}&#x3D;k}\phi(x)^{T}\phi(x^{(i)})+\frac{1}{\gamma_{k}^{2}}\sum_{y^{(i)}&#x3D;k}\sum_{y^{(j)}&#x3D;k}\phi(x^{(i)})^{T}\phi(x^{(j)}) \<br>&amp;&#x3D;k(x,x)-\frac{2}{\gamma_{k}}\sum_{y^{(i)}&#x3D;k}k(x,x^{(i)})+\frac{1}{\gamma_{k}^{2}}\sum_{y^{(i)}&#x3D;k}\sum_{y^{(j)}&#x3D;k}k(x^{(i)},x^{(j)})<br>\end{aligned}<br>$$</p>
<h3 id="Gaussian-Mixture-Model"><a href="#Gaussian-Mixture-Model" class="headerlink" title="Gaussian Mixture Model"></a>Gaussian Mixture Model</h3><p>将复杂的概率密度函数表示为若干简单高斯概率分布密度函数的加权和<br>$$<br>\quad p(x)&#x3D;\sum_{k&#x3D;1}^{K}\alpha_{k}p(x;\mu_{k},\Sigma_{k})<br>$$<br>$\mu_k,\sum_k$分别为第k个高斯概率分布密度函数的参数，$\alpha_k$为加权系数，$\sum_{k&#x3D;1}^K \alpha_k&#x3D;1$</p>
<h4 id="与kmeans区别"><a href="#与kmeans区别" class="headerlink" title="与kmeans区别"></a>与kmeans区别</h4><p>kmeans采用均值表示聚类簇，GMM用一个高维高斯模型表示一个聚类簇</p>
<h4 id="GMM聚类"><a href="#GMM聚类" class="headerlink" title="GMM聚类"></a>GMM聚类</h4><p>$$<br>p(x)&#x3D;\sum_{k&#x3D;1}^K\alpha_kp(x;\mu_k,\Sigma_k)&#x3D;\sum_{k&#x3D;1}^Kp(z&#x3D;k)p(x|z&#x3D;k)<br>$$</p>
<p>$$<br>\alpha_k&#x3D;p(z&#x3D;k)<br>$$</p>
<p>$$<br>p(x|z&#x3D;k)&#x3D;p(x;\mu_k,\Sigma_k)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}\sqrt{|\Sigma_k|}}\exp\left(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right)<br>$$</p>
<p>贝叶斯定理得出后验概率<br>$$<br>p(z^{(j)}&#x3D;k|x^{(j)})&#x3D;\frac{p(x^{(j)}|z^{(j)}&#x3D;k)p(z^{(j)}&#x3D;k)}{\sum_lp(x^{(j)}|z^{(j)}&#x3D;l)p(z^{(j)}&#x3D;l)}&#x3D;\frac{\alpha_kp(x^{(j)};\mu_k,\Sigma_k)}{\sum_l\alpha_lp(x^{(j)};\mu_l,\Sigma_l)}<br>$$<br>得到簇标记后完成聚类</p>
<p>最大化对数似然<br>$$<br>\ell(\theta)&#x3D;\log\prod_{j&#x3D;1}^mp(x^{(j)})&#x3D;\sum_{j&#x3D;1}^m\log\left(\sum_{k&#x3D;1}^K\alpha_kp(x^{(j)};\mu_k,\Sigma_k)\right)<br>$$</p>
<h5 id="求-mu-k"><a href="#求-mu-k" class="headerlink" title="求$\mu_k$"></a>求$\mu_k$</h5><p>$\frac{\partial\ell}{\partial\mu_{k}}&#x3D;0$得到<br>$$<br>\begin{aligned}\sum_{j&#x3D;1}^m\frac{\alpha_kp(x^{(j)};\mu_k,\Sigma_k)}{\sum_{l&#x3D;1}^K\alpha_lp(x^{(j)};\mu_l,\Sigma_l)}(x^{(j)}-\mu_k)&amp;&#x3D;0\end{aligned}<br>$$<br>令<br>$$<br>\begin{aligned}\gamma_{jk}&#x3D;p(z^{(j)}&#x3D;k|x^{(j)})&#x3D;\frac{\alpha_{k}p(x^{(j)};\mu_{k},\Sigma_{k})}{\sum_{l&#x3D;1}^{K}\alpha_{l}p(x^{(j)};\mu_{l},\Sigma_{l})}\end{aligned}<br>$$<br>得到<br>$$<br>\mu_k&#x3D;\frac{\sum_{j&#x3D;1}^m\gamma_{jk}x^{(j)}}{\sum_{j&#x3D;1}^m\gamma_{jk}}<br>$$</p>
<h5 id="求-sum-k"><a href="#求-sum-k" class="headerlink" title="求$\sum_k$"></a>求$\sum_k$</h5><p>$\frac{\partial\ell}{\partial\sum_{k}}&#x3D;0$得到<br>$$<br>\Sigma_k&#x3D;\frac{\sum_{j&#x3D;1}^m\gamma_{jk}(x^{(j)}-\mu_k)(x^{(j)}-\mu_k)^T}{\sum_{j&#x3D;1}^m\gamma_{jk}}<br>$$</p>
<h5 id="求-alpha-k"><a href="#求-alpha-k" class="headerlink" title="求$\alpha_k$"></a>求$\alpha_k$</h5><p>引入拉格朗日乘子，$\sum_k \alpha_k&#x3D;1$<br>$$<br>\ell + \lambda(\sum_{k&#x3D;1}^K \alpha_k -1)<br>$$<br>得到<br>$$<br>\alpha_k&#x3D;\frac{1}{m} \sum_{j&#x3D;1}^m \gamma_{jk}<br>$$</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>已知<br>$$<br>p(x)&#x3D;\sum_{k&#x3D;1}^K\alpha_kp(x;\mu_k,\Sigma_k)&#x3D;\sum_{k&#x3D;1}^Kp(z&#x3D;k)p(x|z&#x3D;k)<br>$$</p>
<p>$$<br>\alpha_k&#x3D;p(z&#x3D;k)<br>$$</p>
<p>$$<br>p(x|z&#x3D;k)&#x3D;p(x;\mu_k,\Sigma_k)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}\sqrt{|\Sigma_k|}}\exp\left(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right)<br>$$</p>
<p>$$<br>\ell(\theta)&#x3D;\log\prod_{j&#x3D;1}^mp(x^{(j)})&#x3D;\sum_{j&#x3D;1}^m\log\left(\sum_{k&#x3D;1}^K\alpha_kp(x^{(j)};\mu_k,\Sigma_k)\right)<br>$$</p>
<p>在测试时计算出：<br>$$<br>p(z^{(j)}&#x3D;k|x^{(j)})&#x3D;\frac{p(x^{(j)}|z^{(j)}&#x3D;k)p(z^{(j)}&#x3D;k)}{\sum_lp(x^{(j)}|z^{(j)}&#x3D;l)p(z^{(j)}&#x3D;l)}&#x3D;\frac{\alpha_kp(x^{(j)};\mu_k,\Sigma_k)}{\sum_l\alpha_lp(x^{(j)};\mu_l,\Sigma_l)}<br>$$<br>计算上式需要求得$\theta$，$\theta$的最佳组合是：<br>$$<br>\theta^{*}&#x3D;\arg\max_{\theta}\ell(\theta)<br>$$<br>通过拉格朗日乘数法，求偏导得到参数：<br>$$<br>\mu_k&#x3D;\frac{\sum_{j&#x3D;1}^m\gamma_{jk}x^{(j)}}{\sum_{j&#x3D;1}^m\gamma_{jk}}<br>$$</p>
<p>$$<br>\Sigma_k&#x3D;\frac{\sum_{j&#x3D;1}^m\gamma_{jk}(x^{(j)}-\mu_k)(x^{(j)}-\mu_k)^T}{\sum_{j&#x3D;1}^m\gamma_{jk}}<br>$$</p>
<p>$$<br>\alpha_k&#x3D;\frac{1}{m} \sum_{j&#x3D;1}^m \gamma_{jk}<br>$$</p>
<h3 id="Expection-Maximization"><a href="#Expection-Maximization" class="headerlink" title="Expection Maximization"></a>Expection Maximization</h3><p>期望最大化</p>
<p>给定m互相独立的训练样本集${x^{(1)},\dots,x^{(m)}}$，期望得到模型的最优参数，采用最大似然估计进行求解，隐变量$z^{(i)}$无法直接进行优化</p>
<ul>
<li>采用策略：构建一个损失函数下界（E），优化下界（M）</li>
</ul>
<p><img src="/imgs/ml/93.png" alt="image-20231208161731660"><br>$$<br>\ell(\theta)&#x3D;\sum_{i}\log p(x^{(i)};\theta)\geq\sum_{i}\sum_{z^{(i)}}Q_{i}(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_{i}(z^{(i)})}<br>$$<br><img src="/imgs/ml/95.png" alt="image-20231208162015653"></p>
<p><img src="/imgs/ml/94.png" alt="image-20231208161916929"></p>
<h4 id="GMM-EM"><a href="#GMM-EM" class="headerlink" title="GMM-EM"></a>GMM-EM</h4><h5 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h5><p>$$<br>\gamma_{ik}&#x3D;Q_{i}(z^{(i)}&#x3D;k)&#x3D;p(z^{(i)}&#x3D;k|x^{(i)};\theta)&#x3D;p(z^{(i)}&#x3D;k|x^{(i)};\alpha,\mu,\Sigma),<br>$$</p>
<h5 id="M-step"><a href="#M-step" class="headerlink" title="M-step"></a>M-step</h5><p>$$<br>\begin{gathered}<br>\sum_{i&#x3D;1}^{m}\sum_{z^{(i)}}Q_{i}(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\alpha,\mu,\Sigma)}{Q_{i}(z^{(i)})} \<br>&#x3D;\sum_{i&#x3D;1}^{m}\sum_{k&#x3D;1}^{K}Q_{i}(z^{(i)}&#x3D;k)\log\frac{p(x^{(i)}|z^{(i)}&#x3D;k;\mu,\Sigma)p(z^{(i)}&#x3D;k;\alpha)}{Q_{i}(z^{(i)}&#x3D;k)} \<br>&#x3D;\sum_{i&#x3D;1}^{m}\sum_{k&#x3D;1}^{K}\gamma_{ik}\log\frac{\frac{1}{(2\pi)^{n&#x2F;2}|\Sigma_{k}|^{1&#x2F;2}}\exp\left(-\frac{1}{2}\big(x^{(i)}-\mu_{k}\big)^{T}\Sigma_{k}^{-1}\big(x^{(i)}-\mu_{k}\big)\right)\cdot\alpha_{k}}{\gamma_{ik}}<br>\end{gathered}<br>$$</p>
<h6 id="mu"><a href="#mu" class="headerlink" title="$\mu$"></a>$\mu$</h6><p>$$<br>\begin{aligned}<br>&amp; \frac{\partial}{\partial\mu_{l}}\sum_{i&#x3D;1}^{m}\sum_{k&#x3D;1}^{K}\gamma_{ik}\log\frac{\frac{1}{(2\pi)^{n&#x2F;2}|\Sigma_{k}|^{1&#x2F;2}}\exp\left(-\frac{1}{2}(x^{(i)}-\mu_{k})^{T}\Sigma_{k}^{-1}(x^{(i)}-\mu_{k})\right)\cdot\alpha_{k}}{\gamma_{ik}}  \<br>&amp;&#x3D;\quad-\frac{\partial}{\partial\mu_{l}}\sum_{i&#x3D;1}^{m}\sum_{k&#x3D;1}^{K}\gamma_{ik}\frac{1}{2}(x^{(i)}-\mu_{k})^{T}\Sigma_{k}^{-1}(x^{(i)}-\mu_{k}) \<br>&amp;&#x3D;\quad\frac{1}{2}\sum_{i&#x3D;1}^{m}\gamma_{il}\frac{\partial}{\partial\mu_{l}}(2\mu_{l}^{T}\Sigma_{l}^{-1}x^{(i)}-\mu_{l}^{T}\Sigma_{l}^{-1}\mu_{l}) \<br>&amp;&#x3D;\quad\sum_{i&#x3D;1}^m\gamma_{il}\left(\Sigma_l^{-1}x^{(i)}-\Sigma_l^{-1}\mu_l\right)&#x3D;0<br>\end{aligned}<br>$$</p>
<p>得到<br>$$<br>\mu_{l}:&#x3D;\frac{\sum_{i&#x3D;1}^{m}\gamma_{il}x^{(i)}}{\sum_{i&#x3D;1}^{m}\gamma_{il}},<br>$$</p>
<h6 id="alpha"><a href="#alpha" class="headerlink" title="$\alpha$"></a>$\alpha$</h6><p>$$<br>\begin{aligned}\mathcal{L}(\alpha)&amp;&#x3D;\sum_{i&#x3D;1}^m\sum_{k&#x3D;1}^K\gamma_{ik}\log\alpha_k+\beta(\sum_{k&#x3D;1}^K\alpha_k-1),\\\frac{\partial}{\partial\alpha_k}\mathcal{L}(\alpha)&amp;&#x3D;\sum_{i&#x3D;1}^m\frac{\gamma_{ik}}{\alpha_k}+\beta&#x3D;0\end{aligned}<br>$$</p>
<p>$$<br>\alpha_{k}:&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}\gamma_{ik}.<br>$$</p>
<h2 id="维数约简"><a href="#维数约简" class="headerlink" title="维数约简"></a>维数约简</h2><ul>
<li>Discover hidden correlations&#x2F;topics<ul>
<li>Words that occur commonly together</li>
</ul>
</li>
<li>Remove redundant and noisy features<ul>
<li>Not all words are useful</li>
</ul>
</li>
<li>Easier storage and processing of the data</li>
<li>interpretation and visualization</li>
</ul>
<p><strong>数据压缩</strong><br>$$<br>\begin{aligned}x^{(1)}&amp;\in\mathbb{R}^2\to z^{(1)}\in\mathbb{R}^1\x^{(2)}&amp;\in\mathbb{R}^2\to z^{(2)}\in\mathbb{R}^1\&amp;\vdots\x^{(m)}&amp;\in\mathbb{R}^2\to z^{(m)}\in\mathbb{R}^1\end{aligned}<br>$$<br><img src="/imgs/ml/96.png" alt="image-20231208172441023"></p>
<p><img src="/imgs/ml/97.png" alt="image-20231208172535325"></p>
<p><strong>投影与投影误差</strong></p>
<p>一维情况$\hat x &#x3D;\frac{u}{||u||^2}(u^Tx)$去近似x，平方投影误差$||x-\hat x||^2&#x3D;(x-\hat x)^T(x-\hat x)$</p>
<p>u为单位向量<br>$$<br>\min_{u:|u|&#x3D;1}\sum_{i&#x3D;1}^m|x^{(i)}-uu^Tx^{(i)}|^2<br>$$</p>
<h3 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析-PCA"></a>主成分分析-PCA</h3><p>从n维降到k维：找到k个方向${u^{(1)},\dots,u^{(k)}}$进行数据投影，使得投影误差最小<br>$$<br>\begin{aligned}<br>|x-uu^{T}x|^{2}&amp; &#x3D;(x-uu^{T}x)^{T}(x-uu^{T}x)  \<br>&amp;&#x3D;x^{T}x-x^{T}uu^{T}x-x^{T}uu^{T}x+x^{T}uu^{T}uu^{T}x \<br>&amp;&#x3D;x^{T}x-x^{T}uu^{T}x<br>\end{aligned}<br>$$<br>找具有最小投影误差的方向等价于如下优化问题（最大化方差）<br>$$<br>\begin{aligned}\max_u&amp;u^T\left(\sum_{i&#x3D;1}^mx^{(i)}x^{(i)^T}\right)u\s.t.&amp;|u|&#x3D;1\end{aligned}<br>$$</p>
<p>$$<br>L&#x3D;u^T\left(\sum_{i&#x3D;1}^mx^{(i)}x^{(i)^T}\right)u-\lambda(u^Tu-1)&#x3D;u^TXX^Tu-\lambda(u^Tu-1)<br>$$</p>
<p>$\frac{\partial L}{\partial u}&#x3D;0$<br>$$<br>XX^Tu&#x3D;\Sigma u&#x3D;\lambda u,\Sigma&#x3D;XX^T<br>$$</p>
<p>$$<br>u^TXX^Tu&#x3D;u^T\lambda u&#x3D;\lambda.<br>$$</p>
<p>如何找到K个投影方向</p>
<p>选择$\Sigma$最大的前面K个特征值所对应的特征向量，因此$\Sigma$是对称阵，这些特征向量必定两两正交，构成K维子空间的一组标准正交基。$x^{(i)}$对应的投影为<br>$$<br>\left.z^{(i)}&#x3D;\left[\begin{array}{cc}u^{(1)}{}^Tx^{(i)}\u^{(2)}{}^Tx^{(i)}\\vdots\u^{(K)}{}^Tx^{(i)}\end{array}\right.\right]&#x3D;U_K^Tx^{(i)}\in\mathbb{R}^K.<br>$$</p>
<p>$$<br>U_{K}&#x3D;[u^{(1)},u^{(2)},\cdots,u^{(K)}]\in\mathbb{R}^{n\times K}<br>$$</p>
<h4 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h4><p>输入：样本集${ x^{(1)},\dots,x^{(m)} }$，低维空间的维度K</p>
<p>过程：</p>
<ol>
<li>对所有样本进行中心化处理$x^{(i)}\leftarrow x^{(i)}-\frac{1}{m}\sum_{j}x^{(j)}$</li>
<li>计算样本的协方差矩阵$\Sigma&#x3D;XX^{T}$</li>
<li>对协方差矩阵$\Sigma$进行特征值分解</li>
<li>选取最大的K个特征值对应的特征向量${ u^{(1)},\dots,u^{(K)} }$</li>
</ol>
<h4 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p>
<p>奇异值分解定理<br>$$<br>X&#x3D;USV^{T}<br>$$<br>左奇异向量<br>$$<br>U\in\mathbb{R}^{m\times r},U^TU&#x3D;I<br>$$<br>奇异值<br>$$<br>S&#x3D;\mathrm{diag}(\sigma_{1},\sigma_{2},\cdots,\sigma_{r})\in\mathbb{R}^{r\times n}<br>$$</p>
<p>$$<br>\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_r&gt;0<br>$$</p>
<p>右奇异向量<br>$$<br>V\in\mathbb{R}^{n\times r},V^TV&#x3D;I<br>$$</p>
<p>$$<br>X&#x3D;USV^T&#x3D;\sum_i\sigma_iu_iv_i^T<br>$$</p>
<p><img src="/imgs/ml/98.png" alt="image-20231220171735636"></p>
<p>$X&#x3D;[x^{(1)},x^{(2)},\cdots,x^{(m)}]\in\mathbb{R}^{n\times m},:XX^T\in\mathbb{R}^{n\times n},\quad X^TX\in\mathbb{R}^{m\times m},\quad$若特征的维数$n$大于样本个数$m$, 即$n\geq m$, 可以把求解$XX^T$的特征分解问题转化为求$X^TX$的特征分解问题。</p>
<p>$$<br>X&#x3D;USV^{T}\quad XX^{T}&#x3D;USV^{T}VSU^{T}&#x3D;US^{2}U^{T}<br>$$</p>
<p>$XX^TU&#x3D;US^2\Rightarrow$ Left Singular Vectors $U$的列是$\Sigma&#x3D;XX^T$的特征向量，而Σ的特征值和X的奇异值的关系为：$\lambda_i&#x3D;\sigma_i^2$<br>同理，有$X^TXV&#x3D;VS^2\Rightarrow$ Right Singular Vectors $V$的列是$X^TX$ 的特征向量，对应特征值和$X$的奇异值的关系为$:\lambda_i&#x3D;\sigma_i^2$<br>$$<br>\begin{aligned}XX^TU&amp;&#x3D;US^2\text{即:}\quad XX^Tu_i&#x3D;\sigma_i^2u_i\X^TXV&amp;&#x3D;VS^2\text{即:}\quad X^TXv_i&#x3D;\sigma_i^2v_i\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned}\text{将}X&#x3D;USV^T&#x3D;\sum_j\sigma_ju_jv_j^T\text{代入,}Xv_i&#x3D;\sum_j\sigma_ju_jv_j^Tv_i&#x3D;\sigma_iu_i\\boxed{u_i&#x3D;\frac{1}{\sigma_i}Xv_i&#x3D;\frac{1}{\sqrt{\lambda_i}}Xv_i}\end{aligned}<br>$$</p>
<p><img src="/imgs/ml/99.png" alt="image-20231220221537243"></p>
<p><img src="/imgs/ml/100.png" alt="image-20231220222043182"><br>$$<br>\frac{\sum_{i&#x3D;1}^m|x^{(i)}-\hat{x}^{(i)}|^2}{\sum_{i&#x3D;1}^m|x^{(i)}|^2}&#x3D;1-\frac{\sum_{j&#x3D;1}^K\lambda_j}{\sum_{j&#x3D;1}^n\lambda_j}\leq\epsilon<br>$$<br><img src="/imgs/ml/101.png" alt="image-20231220222132700"></p>
<h4 id="PCA的不好使用-防止过拟合"><a href="#PCA的不好使用-防止过拟合" class="headerlink" title="PCA的不好使用-防止过拟合"></a>PCA的不好使用-防止过拟合</h4><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/47121788?sort=created">为什么PCA不被推荐用来避免过拟合？ - 知乎 (zhihu.com)</a></p>
<p>由于PCA整个方案都没用到y，所以过拟合问题并不能用PCA来降维攻击</p>
<p>PCA是unsupervised learning 输出结果不可靠</p>
<p>PCA降维损失信息</p>
<p>老实用正则化</p>
<h4 id="kernel-PCA"><a href="#kernel-PCA" class="headerlink" title="kernel PCA"></a>kernel PCA</h4><p>$$<br>XX^{T}u&#x3D;\lambda u\Rightarrow\Phi\Phi^{T}u&#x3D;\lambda u<br>$$</p>
<p>$$<br>\Phi&#x3D;[\phi(x^{(1)}),\phi(x^{(2)}),\cdots,\phi(x^{(m)})]<br>$$</p>
<p>由于$\phi(x)$的维度未知，无法直接对$\Phi\Phi^T$进行特征值分解，转而求$\Phi^T\Phi$的特征分解，然后根据Left Singular Vectors和Right Singular Vectors关系间接求解出$\Phi\Phi^T$的特征向量.<br>$$<br>u_{i}&#x3D;\frac{1}{\sqrt{\lambda_{i}}}\Phi v<br>$$</p>
<p>$$<br>\begin{aligned}<br>\Phi^{T}\Phi &amp; &#x3D;\left[\phi(x^{(1)}),\phi(x^{(2)}),\cdots,\phi(x^{(m)})\right]^T\left[\phi(x^{(1)}),\phi(x^{(2)}),\cdots,\phi(x^{(m)})\right]  \<br>&amp;\left.&#x3D;\left[\begin{array}{ccc}\phi(x^{(1)})^T\phi(x^{(1)})&amp;\cdots&amp;\phi(x^{(1)})^T\phi(x^{(m)})\\phi(x^{(2)})^T\phi(x^{(1)})&amp;\cdots&amp;\phi(x^{(2)})^T\phi(x^{(m)})\\vdots&amp;\cdots&amp;\vdots\\phi(x^{(m)})^T\phi(x^{(1)})&amp;\cdots&amp;\phi(x^{(m)})^T\phi(x^{(m)})\end{array}\right.\right] \<br>&amp;\left.&#x3D;\left[\begin{array}{ccc}k(x^{(1)},x^{(1)})&amp;\cdots&amp;k(x^{(1)},x^{(m)})\\vdots&amp;\cdots&amp;\vdots\k(x^{(m)},x^{(1)})&amp;\cdots&amp;k(x^{(m)},x^{(m)})\end{array}\right.\right]&#x3D;\mathbf{K}<br>\end{aligned}<br>$$</p>
<p><img src="/imgs/ml/102.png" alt="image-20231220223714069"></p>
<h3 id="线性鉴别分析LDA"><a href="#线性鉴别分析LDA" class="headerlink" title="线性鉴别分析LDA"></a>线性鉴别分析LDA</h3><ul>
<li>PCA是根据样本投影后数据的方差来选择投影方向的（最优重构）</li>
<li>但对于分类问题而言，这样的投影反而有可能使得数据更加无法划分</li>
<li>例: 识别字母“O”和“Q”</li>
</ul>
<p>$$<br>J_{F}(w)&#x3D;\frac{w^{T}S_{b}w}{w^{T}S_{w}w}<br>$$</p>
<p>$S_b,S_w$分别为类间散布矩阵、类间散布矩阵<br>$$<br>\begin{aligned}<br>&amp;S_{b} &#x3D;(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{T}  \<br>&amp;S_{w} &#x3D;\sum_{y^{(i)}&#x3D;-1}(x^{(i)}-\mu_{1})(x^{(i)}-\mu_{1})^{T}+\sum_{y^{(i)}&#x3D;+1}(x^{(i)}-\mu_{2})(x^{(i)}-\mu_{2})^{T}<br>\end{aligned}<br>$$<br>$J_{F}(w)$为广义瑞丽商</p>
<p><img src="/imgs/ml/103.png" alt="image-20231220225019504"></p>
<p>投影方向w满足以下广义特征值问题<br>$$<br>S_{b}w&#x3D;\lambda S_{w}w<br>$$<br><img src="/imgs/ml/104.png" alt="image-20231220225114325"></p>
<p><img src="/imgs/ml/105.png" alt="image-20231220225137417"></p>
<p><img src="/imgs/ml/106.png" alt="image-20231220225157963"></p>
<p><img src="/imgs/ml/107.png" alt="image-20231220225220881"></p>
<p><img src="/imgs/ml/108.png" alt="image-20231220225243777"></p>
<p><img src="/imgs/ml/109.png" alt="image-20231220225412736"></p>
<h3 id="流形学习"><a href="#流形学习" class="headerlink" title="流形学习"></a>流形学习</h3><p>Manifold Learning (or non-linear dimensionality reduction) embeds data that originally lies in a high dimensional space in a lower dimensional space, while preserving characteristic properties.</p>
<p>a manifold is a topological space that locally resembles Euclidean space near each point</p>
<p><img src="/imgs/ml/110.png" alt="image-20231220225536627"></p>
<h4 id="Locally-Linear-Embedding"><a href="#Locally-Linear-Embedding" class="headerlink" title="Locally Linear Embedding"></a>Locally Linear Embedding</h4><p><img src="/imgs/ml/111.png" alt="image-20231220225619944"></p>
<p><img src="/imgs/ml/112.png" alt="image-20231220225645880"></p>
<p><img src="/imgs/ml/113.png" alt="image-20231220225704684"></p>
<h4 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h4><p><img src="/imgs/ml/114.png" alt="image-20231220225838569"></p>
<p><img src="/imgs/ml/115.png" alt="image-20231220225900396"></p>
<h4 id="ISOMAP"><a href="#ISOMAP" class="headerlink" title="ISOMAP"></a>ISOMAP</h4><p><img src="/imgs/ml/116.png" alt="image-20231220225926123"></p>
<p><img src="/imgs/ml/117.png" alt="image-20231220225954786"></p>
<p><img src="/imgs/ml/118.png" alt="image-20231220230018108"></p>
<h4 id="T-distributed-Stochastic-Neighbor-Embedding-t-SNE"><a href="#T-distributed-Stochastic-Neighbor-Embedding-t-SNE" class="headerlink" title="T-distributed Stochastic Neighbor Embedding (t-SNE)"></a>T-distributed Stochastic Neighbor Embedding (t-SNE)</h4><p><img src="/imgs/ml/119.png" alt="image-20231220230104657"></p>
<p><img src="/imgs/ml/120.png" alt="image-20231220230230186"></p>
<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p><img src="/imgs/ml/121.png" alt="image-20240108232300380"></p>
<p>强化学习的特性</p>
<ul>
<li>Reward delay</li>
</ul>
<p><img src="/imgs/ml/122.png" alt="image-20240108232513891"></p>
<h2 id="系统设计与应用"><a href="#系统设计与应用" class="headerlink" title="系统设计与应用"></a>系统设计与应用</h2><h3 id="错误分析-x2F-算法评估"><a href="#错误分析-x2F-算法评估" class="headerlink" title="错误分析&#x2F;算法评估"></a>错误分析&#x2F;算法评估</h3><p><img src="/imgs/ml/123.png" alt="image-20240108234426103"><br>$$<br>\begin{aligned}<br>&amp;精准率precision&#x3D;\frac{TP}{\hat{P}}&#x3D;\frac{TP}{TP+FP} \<br>&amp;召回率recall&#x3D;\frac{TP}{P}&#x3D;\frac{TP}{TP+FN} \<br>&amp;特异度specificity&#x3D;\frac{TN}{N}&#x3D;\frac{TN}{FP+TN} \<br>&amp;正确率accuracy &#x3D;\frac{TP+TN}{P+N}<br>\end{aligned}<br>$$<br><img src="/imgs/ml/124.png" alt="image-20240108234937224"></p>
<h2 id="lyx的总结"><a href="#lyx的总结" class="headerlink" title="lyx的总结"></a>lyx的总结</h2><p>复习重点</p>
<p>作业</p>
<p>SVM支持向量边界KTT条件，<del>回归算了（）</del></p>
<p>回归</p>
<p>分类</p>
<p>正则化</p>
<p>非线性</p>
<p>神经网络</p>
<p>网络前向传播反向传播</p>
<p>决策树 剪枝（）</p>
<p>集成学习 分类器错误率第二个比第一个高</p>
<p>维数约简 PCA</p>
<p><del>流形学习</del>，不会计算题</p>
<p>应用评价</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">J&Ocean</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jiang-wu-19.github.io/2023/09/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://jiang-wu-19.github.io/2023/09/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">J&Ocean</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">机器学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">请我喝杯奶茶吧~</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.jpg" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'Nv6Wx6PAYH30bfcx7u0eDWHy-gzGzoHsz',
        appKey: 'IhPcpC3fDP8Ro7eaPakG2vSt',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/09/27/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/23.jpg" class="responsive-img" alt="计算机体系结构笔记">
                        
                        <span class="card-title">计算机体系结构笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            2023秋-计算机体系结构
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-09-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%AC%94%E8%AE%B0/" class="post-category">
                                    笔记
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">
                        <span class="chip bg-color">计算机体系结构</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/09/07/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E8%AF%BE%E5%A4%96%E6%80%9D%E8%80%83%E9%A2%980905/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="体系结构课外思考题0905">
                        
                        <span class="card-title">体系结构课外思考题0905</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            体系结构中的一些“墙”
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E6%80%9D%E8%80%83%E9%A2%98/" class="post-category">
                                    思考题
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%86%85%E5%AD%98%E5%A2%99/">
                        <span class="chip bg-color">内存墙</span>
                    </a>
                    
                    <a href="/tags/%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%A2%99/">
                        <span class="chip bg-color">可靠性墙</span>
                    </a>
                    
                    <a href="/tags/%E5%8F%AF%E7%BC%96%E7%A8%8B%E6%80%A7%E5%A2%99/">
                        <span class="chip bg-color">可编程性墙</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: J&amp;Ocean BLOG<br />'
            + '文章作者: J&amp;Ocean<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2024</span>
            
            <span id="year">2023</span>
            <a href="/about" target="_blank">J&Ocean</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">268.5k</span>&nbsp;字
            
            
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2023";
                    var startMonth = "7";
                    var startDate = "2";
                    var startHour = "12";
                    var startMinute = "0";
                    var startSecond = "0";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link ">
    <a href="https://github.com/JIANG-Wu-19" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:wujiang0319@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=870027163" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 870027163" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/people/bei-wei-xiao-wu-32" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/bei-wei-xiao-wu-32" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
